{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.694412Z",
     "start_time": "2024-12-24T14:48:58.230867Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "# ============================================================\n",
    "# Data Generation (This does the same thing as the genimages.py provided)\n",
    "# ============================================================\n",
    "np.random.seed(0)\n",
    "\n",
    "features = [\n",
    "    [0, 0, 1, 0,\n",
    "     0, 1, 1, 1,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [1, 0, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     1, 1, 0, 0,\n",
    "     1, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 1, 1, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1],\n",
    "]\n",
    "\n",
    "num_samples = 2000\n",
    "num_features = 16\n",
    "K_true = len(features)\n",
    "\n",
    "feature_weights = 0.5 + np.random.rand(K_true, 1) * 0.5\n",
    "mu_true = np.array([weight * feat for weight, feat in zip(feature_weights, features)])\n",
    "latent_factors = (np.random.rand(num_samples, K_true) < 0.3).astype(float)\n",
    "data = latent_factors @ mu_true + np.random.randn(num_samples, num_features)*0.1\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.704928Z",
     "start_time": "2024-12-24T14:48:58.699895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianPrior:\n",
    "    def __init__(self, a: float, b: float, d: int, k: int):\n",
    "        \"\"\"\n",
    "        Initialize the Gaussian prior for the mu matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : float\n",
    "            Alpha parameter of the Gamma prior.\n",
    "        b : float\n",
    "            Beta parameter of the Gamma prior.\n",
    "        d : int\n",
    "            Number of dimensions in the observed data.\n",
    "        k : int\n",
    "            Number of latent variables.\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.mu = np.zeros((d, k))\n",
    "        self.alpha = np.ones(k)\n",
    "        self.w_covariance = np.zeros((k, k))\n",
    "\n",
    "    def mu_k(self, latent_factor: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retrieve the column vector of the mu matrix corresponding to a specific latent factor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_factor : int\n",
    "            Index of the latent factor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Column vector of mu, shape (d, 1).\n",
    "        \"\"\"\n",
    "        return self.mu[:, latent_factor:latent_factor + 1]\n",
    "\n",
    "    def w_d(self, dimension: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retrieve the row vector of the mu matrix corresponding to a specific data dimension.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dimension : int\n",
    "            Index of the data dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Row vector of mu, shape (1, k).\n",
    "        \"\"\"\n",
    "        return self.mu[dimension:dimension + 1, :]\n",
    "\n",
    "    @property\n",
    "    def a_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the precision matrix for the weight vector w_d.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Precision matrix, shape (k, k).\n",
    "        \"\"\"\n",
    "        return np.diag(self.alpha)"
   ],
   "id": "e18c277b36e77ab2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.911708Z",
     "start_time": "2024-12-24T14:48:58.898491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VariationalBayes:\n",
    "    def __init__(self, mu: 'GaussianPrior', variance: float, pi: np.ndarray):\n",
    "        \"\"\"\n",
    "        Variational Bayes implementation with a Gaussian prior on mu.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : GaussianPrior\n",
    "            Gaussian prior on latent features.\n",
    "        variance : float\n",
    "            Gaussian noise parameter.\n",
    "        pi : np.ndarray\n",
    "            Vector of prior probabilities, shape (1, number_of_latent_variables).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gaussian_prior = mu\n",
    "        self._variance = variance\n",
    "        self._pi = pi\n",
    "\n",
    "    @property\n",
    "    def log_pi(self) -> np.ndarray:\n",
    "        \"\"\"Compute the natural logarithm of the prior probabilities.\"\"\"\n",
    "        return np.log(self.pi)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_pi(self) -> np.ndarray:\n",
    "        \"\"\"Compute the natural logarithm of the complement of the prior probabilities.\"\"\"\n",
    "        return np.log(1 - self.pi)\n",
    "\n",
    "    @property\n",
    "    def variance(self) -> float:\n",
    "        \"\"\"Get the variance of the Gaussian noise.\"\"\"\n",
    "        return self._variance\n",
    "\n",
    "    @property\n",
    "    def pi(self) -> np.ndarray:\n",
    "        \"\"\"Get the vector of prior probabilities.\"\"\"\n",
    "        return self._pi\n",
    "\n",
    "    @property\n",
    "    def mu(self) -> np.ndarray:\n",
    "        \"\"\"Get the mean matrix of the Gaussian prior.\"\"\"\n",
    "        return self.gaussian_prior.mu\n",
    "\n",
    "    @property\n",
    "    def d(self) -> int:\n",
    "        \"\"\"Get the dimensionality of the observed data.\"\"\"\n",
    "        return self.mu.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"Get the number of latent variables.\"\"\"\n",
    "        return self.mu.shape[1]\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        \"\"\"Get the precision (inverse of variance) of the Gaussian noise.\"\"\"\n",
    "        return 1.0 / self.variance\n",
    "\n",
    "    def mu_exclude(self, exclude_latent_index: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Exclude a specific latent variable index from the mean matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        exclude_latent_index : int\n",
    "            Index of the latent variable to exclude.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Mean matrix excluding the specified latent variable,\n",
    "            shape (number_of_dimensions, number_of_latent_variables - 1).\n",
    "        \"\"\"\n",
    "        return np.concatenate(\n",
    "            (\n",
    "                self.mu[:, :exclude_latent_index],\n",
    "                self.mu[:, exclude_latent_index + 1:]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_maximisation_parameters(\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: 'MeanFieldApproximation'\n",
    "    ) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate M-step parameters for the variational approximation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (number_of_points, number_of_dimensions).\n",
    "        binary_latent_factor_approximation : MeanFieldApproximation\n",
    "            Mean field approximation object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, np.ndarray]\n",
    "            Updated parameters: (means, variance, prior probabilities).\n",
    "        \"\"\"\n",
    "        return m_step(\n",
    "            X=x,\n",
    "            ES=binary_latent_factor_approximation.expectation_s,\n",
    "            ESS=binary_latent_factor_approximation.expectation_ss,\n",
    "        )\n",
    "\n",
    "    def _update_w_covariance(\n",
    "        self,\n",
    "        binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Update the covariance matrix for the latent feature weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        binary_latent_factor_approximation : MeanFieldApproximation\n",
    "            Mean field approximation object.\n",
    "        \"\"\"\n",
    "        a_matrix = self.gaussian_prior.a_matrix\n",
    "        expectation_ss = binary_latent_factor_approximation.expectation_ss\n",
    "        covariance_matrix = a_matrix + self.precision * expectation_ss\n",
    "        self.gaussian_prior.w_covariance = np.linalg.inv(covariance_matrix)\n",
    "\n",
    "    def _update_w_mean(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    "        dimension_index: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Update the mean vector for the latent feature weights for a specific dimension.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (number_of_points, number_of_dimensions).\n",
    "        binary_latent_factor_approximation : MeanFieldApproximation\n",
    "            Mean field approximation object.\n",
    "        dimension_index : int\n",
    "            Index of the data dimension to update.\n",
    "        \"\"\"\n",
    "        expectation_s = binary_latent_factor_approximation.expectation_s\n",
    "        data_column = x[:, dimension_index : dimension_index + 1]\n",
    "        updated_mean = (\n",
    "            self.gaussian_prior.w_covariance\n",
    "            @ (self.precision * expectation_s.T @ data_column)\n",
    "        ).T\n",
    "        self.gaussian_prior.mu[dimension_index : dimension_index + 1, :] = updated_mean\n",
    "\n",
    "    def _hyper_maximisation_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Hyperparameter maximisation step to update alpha, which parameterizes\n",
    "        the covariance matrix of the Gaussian prior on mu.\n",
    "        \"\"\"\n",
    "        for latent_idx in range(self.k):\n",
    "            mu_k = self.gaussian_prior.mu_k(latent_idx)\n",
    "            w_cov_kk = self.gaussian_prior.w_covariance[latent_idx, latent_idx]\n",
    "            numerator = 2 * self.gaussian_prior.a + self.d - 2\n",
    "            denominator = (\n",
    "                2 * self.gaussian_prior.b\n",
    "                + np.sum(mu_k ** 2)\n",
    "                + self.d * w_cov_kk\n",
    "            )\n",
    "            self.gaussian_prior.alpha[latent_idx] = numerator / denominator\n",
    "\n",
    "    def maximisation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform the maximisation step, which includes the standard M-step,\n",
    "        updates to the posterior distribution of mu, and a hyperparameter step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (number_of_points, number_of_dimensions).\n",
    "        binary_latent_factor_approximation : MeanFieldApproximation\n",
    "            Mean field approximation object.\n",
    "        \"\"\"\n",
    "        _, sigma, pi = self.calculate_maximisation_parameters(\n",
    "            x, binary_latent_factor_approximation\n",
    "        )\n",
    "        self._variance = sigma ** 2\n",
    "        self._pi = pi\n",
    "\n",
    "        self._update_w_covariance(binary_latent_factor_approximation)\n",
    "\n",
    "        for dim_idx in range(self.d):\n",
    "            self._update_w_mean(x, binary_latent_factor_approximation, dim_idx)\n",
    "\n",
    "        self._hyper_maximisation_step()"
   ],
   "id": "c3681a379c03ac70",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.927921Z",
     "start_time": "2024-12-24T14:48:58.922528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def m_step(X, ES, ESS):\n",
    "    \"\"\"\n",
    "    mu, sigma, pie = MStep(X,ES,ESS)\n",
    "\n",
    "    Inputs:\n",
    "    -----------------\n",
    "           X: shape (N, D) data matrix\n",
    "          ES: shape (N, K) E_q[s]\n",
    "         ESS: shape (K, K) sum over data points of E_q[ss'] (N, K, K)\n",
    "                           if E_q[ss'] is provided, the sum over N is done for you.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "          mu: shape (D, K) matrix of means in p(y|{s_i},mu,sigma)\n",
    "       sigma: shape (,)    standard deviation in same\n",
    "         pie: shape (1, K) vector of parameters specifying generative distribution for s\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    if ES.shape[0] != N:\n",
    "        raise TypeError('ES must have the same number of rows as X')\n",
    "    K = ES.shape[1]\n",
    "    if ESS.shape == (N, K, K):\n",
    "        ESS = np.sum(ESS, axis=0)\n",
    "    if ESS.shape != (K, K):\n",
    "        raise TypeError('ESS must be square and have the same number of columns as ES')\n",
    "\n",
    "    mu = np.dot(np.dot(np.linalg.inv(ESS), ES.T), X).T\n",
    "    sigma = np.sqrt((np.trace(np.dot(X.T, X)) + np.trace(np.dot(np.dot(mu.T, mu), ESS))\n",
    "                     - 2 * np.trace(np.dot(np.dot(ES.T, X), mu))) / (N * D))\n",
    "    pie = np.mean(ES, axis=0, keepdims=True)\n",
    "\n",
    "    return mu, sigma, pie\n"
   ],
   "id": "3677ff1db858346d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.955552Z",
     "start_time": "2024-12-24T14:48:58.938604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MeanFieldApproximation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_matrix: np.ndarray,\n",
    "        max_steps: int,\n",
    "        convergence_criterion: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Mean Field Approximation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lambda_matrix : np.ndarray\n",
    "            Parameters of the variational approximation, shape (n, k),\n",
    "            where n is the number of data points and k is the number of latent variables.\n",
    "        max_steps : int\n",
    "            Maximum number of iterations for the variational expectation step.\n",
    "        convergence_criterion : float\n",
    "            Threshold for convergence based on the change in free energy.\n",
    "        \"\"\"\n",
    "        self._lambda_matrix = lambda_matrix\n",
    "        self.max_steps = max_steps\n",
    "        self.convergence_criterion = convergence_criterion\n",
    "\n",
    "    @property\n",
    "    def lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the lambda matrix of the variational approximation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The lambda matrix, shape (n, k).\n",
    "        \"\"\"\n",
    "        return self._lambda_matrix\n",
    "\n",
    "    @lambda_matrix.setter\n",
    "    def lambda_matrix(self, value: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the lambda matrix of the variational approximation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : np.ndarray\n",
    "            New lambda matrix, shape (n, k).\n",
    "        \"\"\"\n",
    "        self._lambda_matrix = value\n",
    "\n",
    "    def lambda_matrix_exclude(self, exclude_latent_index: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Exclude a specific latent variable index from the lambda matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        exclude_latent_index : int\n",
    "            Index of the latent variable to exclude.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Lambda matrix excluding the specified latent variable,\n",
    "            shape (n, k - 1).\n",
    "        \"\"\"\n",
    "        return np.concatenate(\n",
    "            (\n",
    "                self.lambda_matrix[:, :exclude_latent_index],\n",
    "                self.lambda_matrix[:, exclude_latent_index + 1 :],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def expectation_s(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the expectation of the latent variables (E[s]).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Expectation matrix, shape (n, k).\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix\n",
    "\n",
    "    @property\n",
    "    def expectation_ss(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the expectation of the product of latent variables (E[s s^T]).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Expectation matrix, shape (k, k).\n",
    "        \"\"\"\n",
    "        ess = self.lambda_matrix.T @ self.lambda_matrix\n",
    "        np.fill_diagonal(ess, self.lambda_matrix.sum(axis=0))\n",
    "        return ess\n",
    "\n",
    "    @property\n",
    "    def log_lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the natural logarithm of the lambda matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Log-transformed lambda matrix, shape (n, k).\n",
    "        \"\"\"\n",
    "        return np.log(self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the natural logarithm of (1 - lambda matrix).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Log-transformed complement of lambda matrix, shape (n, k).\n",
    "        \"\"\"\n",
    "        return np.log(1 - self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of data points.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of data points (n).\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of latent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of latent variables (k).\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[1]\n",
    "\n",
    "    def compute_free_energy(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: \"VariationalBayes\",\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute the free energy associated with the current EM parameters and data x.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (n, d), where n is the number of data points and d is the number of dimensions.\n",
    "        binary_latent_factor_model : VariationalBayes\n",
    "            A binary latent factor model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Average free energy per data point.\n",
    "        \"\"\"\n",
    "        expectation_log_p_x_s_given_theta = self._compute_expectation_log_p_x_s_given_theta(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "        approximation_model_entropy = self._compute_approximation_model_entropy()\n",
    "        return (\n",
    "            expectation_log_p_x_s_given_theta + approximation_model_entropy\n",
    "        ) / self.n\n",
    "\n",
    "    def _partial_expectation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: \"VariationalBayes\",\n",
    "        latent_factor: int,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform a partial variational E-step for a specific latent factor across all data points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (n, d).\n",
    "        binary_latent_factor_model : VariationalBayes\n",
    "            A binary latent factor model.\n",
    "        latent_factor : int\n",
    "            Index of the latent factor to update.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Updated lambda vector for the specified latent factor, shape (n, 1).\n",
    "        \"\"\"\n",
    "        lambda_matrix_excluded = self.lambda_matrix_exclude(latent_factor)\n",
    "        mu_excluded = binary_latent_factor_model.mu_exclude(latent_factor)\n",
    "\n",
    "        mu_latent = binary_latent_factor_model.mu[:, latent_factor]\n",
    "\n",
    "        # Compute the proportion of the expectation log p(x, s | theta)\n",
    "        partial_log_prob = (\n",
    "            binary_latent_factor_model.precision\n",
    "            * (\n",
    "                x\n",
    "                - 0.5 * mu_latent.T\n",
    "                - lambda_matrix_excluded @ mu_excluded.T\n",
    "            )\n",
    "            @ mu_latent\n",
    "        )\n",
    "\n",
    "        # Compute the log-odds for the latent factor\n",
    "        log_odds = np.log(\n",
    "            binary_latent_factor_model.pi[0, latent_factor]\n",
    "            / (1 - binary_latent_factor_model.pi[0, latent_factor])\n",
    "        )\n",
    "\n",
    "        # Total partial expectation\n",
    "        total_partial_log_prob = partial_log_prob + log_odds\n",
    "\n",
    "        # Update lambda using the sigmoid function\n",
    "        lambda_vector = 1 / (\n",
    "            1 + np.exp(-total_partial_log_prob)\n",
    "        )\n",
    "        lambda_vector = np.clip(lambda_vector, 1e-10, 1 - 1e-10)\n",
    "        return lambda_vector\n",
    "\n",
    "    def _compute_expectation_log_p_x_s_given_theta(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: \"VariationalBayes\",\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute the expectation of log P(X, S | theta).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (n, d).\n",
    "        binary_latent_factor_model : VariationalBayes\n",
    "            A binary latent factor model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Expectation of log P(X, S | theta).\n",
    "        \"\"\"\n",
    "        mu_lambda = self.lambda_matrix @ binary_latent_factor_model.mu.T\n",
    "\n",
    "        # Compute E[s_i s_j] * mu_i * mu_j\n",
    "        expectation_s_i_s_j_mu_i_mu_j = np.multiply(\n",
    "            self.lambda_matrix.T @ self.lambda_matrix,\n",
    "            binary_latent_factor_model.mu.T @ binary_latent_factor_model.mu,\n",
    "        )\n",
    "\n",
    "        # Calculate the expectation log P(X | S, theta)\n",
    "        expectation_log_p_x_given_s_theta = (\n",
    "            - (self.n * binary_latent_factor_model.d / 2)\n",
    "            * np.log(2 * np.pi * binary_latent_factor_model.variance)\n",
    "            - 0.5 * binary_latent_factor_model.precision\n",
    "            * (\n",
    "                np.sum(x ** 2)\n",
    "                - 2 * np.sum(x * mu_lambda)\n",
    "                + np.sum(expectation_s_i_s_j_mu_i_mu_j)\n",
    "                - np.trace(expectation_s_i_s_j_mu_i_mu_j)\n",
    "                + np.sum(\n",
    "                    self.lambda_matrix\n",
    "                    @ np.multiply(\n",
    "                        binary_latent_factor_model.mu,\n",
    "                        binary_latent_factor_model.mu,\n",
    "                    ).T\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate the expectation log P(S | theta)\n",
    "        expectation_log_p_s_given_theta = np.sum(\n",
    "            self.lambda_matrix * binary_latent_factor_model.log_pi\n",
    "            + (1 - self.lambda_matrix) * binary_latent_factor_model.log_one_minus_pi\n",
    "        )\n",
    "\n",
    "        return expectation_log_p_x_given_s_theta + expectation_log_p_s_given_theta\n",
    "\n",
    "    def _compute_approximation_model_entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the entropy of the variational approximation model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Model entropy.\n",
    "        \"\"\"\n",
    "        entropy = -np.sum(\n",
    "            self.lambda_matrix * self.log_lambda_matrix\n",
    "            + (1 - self.lambda_matrix) * self.log_one_minus_lambda_matrix\n",
    "        )\n",
    "        return entropy\n",
    "\n",
    "    def variational_expectation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: \"VariationalBayes\",\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Perform the variational expectation step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Data matrix, shape (n, d).\n",
    "        binary_latent_factor_model : VariationalBayes\n",
    "            A binary latent factor model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            List of free energy values at each step.\n",
    "        \"\"\"\n",
    "        free_energy = [self.compute_free_energy(x, binary_latent_factor_model)]\n",
    "        for step in range(self.max_steps):\n",
    "            for latent_factor in range(binary_latent_factor_model.k):\n",
    "                updated_lambda = self._partial_expectation_step(\n",
    "                    x, binary_latent_factor_model, latent_factor\n",
    "                )\n",
    "                self.lambda_matrix[:, latent_factor] = updated_lambda\n",
    "                free_energy.append(\n",
    "                    self.compute_free_energy(x, binary_latent_factor_model)\n",
    "                )\n",
    "                energy_change = free_energy[-1] - free_energy[-2]\n",
    "                if energy_change <= self.convergence_criterion:\n",
    "                    break\n",
    "            if free_energy[-1] - free_energy[-2] <= self.convergence_criterion:\n",
    "                break\n",
    "        return free_energy\n",
    "\n",
    "\n",
    "def init_mean_field_approximation(\n",
    "    k: int, n: int, max_steps: int, convergence_criterion: float\n",
    ") -> MeanFieldApproximation:\n",
    "    \"\"\"\n",
    "    Initialize a MeanFieldApproximation instance with random lambda matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of latent variables.\n",
    "    n : int\n",
    "        Number of data points.\n",
    "    max_steps : int\n",
    "        Maximum number of iterations for the variational expectation step.\n",
    "    convergence_criterion : float\n",
    "        Threshold for convergence based on the change in free energy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MeanFieldApproximation\n",
    "        Initialized MeanFieldApproximation instance.\n",
    "    \"\"\"\n",
    "    lambda_matrix = np.random.rand(n, k)\n",
    "    return MeanFieldApproximation(\n",
    "        lambda_matrix=lambda_matrix,\n",
    "        max_steps=max_steps,\n",
    "        convergence_criterion=convergence_criterion,\n",
    "    )"
   ],
   "id": "a49362855bb0a02e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.969800Z",
     "start_time": "2024-12-24T14:48:58.964869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def automatic_relevance_determination(\n",
    "    x: np.ndarray,\n",
    "    binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    "    a_parameter: float,\n",
    "    b_parameter: float,\n",
    "    k: int,\n",
    "    em_iterations: int,\n",
    ") -> Tuple['VariationalBayes', List[float]]:\n",
    "    \"\"\"\n",
    "    Perform Automatic Relevance Determination (ARD) using Variational Bayes.\n",
    "\n",
    "    This function initializes the Variational Bayes model with a Gaussian prior,\n",
    "    performs the Expectation-Maximization (EM) algorithm to optimize the model parameters,\n",
    "    and returns the optimized model along with the history of free energy values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_dimensions), where n_samples is the number of data points\n",
    "        and n_dimensions is the number of observed dimensions.\n",
    "    binary_latent_factor_approximation : MeanFieldApproximation\n",
    "        An instance of MeanFieldApproximation representing the current variational approximation\n",
    "        of the binary latent factors.\n",
    "    a_parameter : float\n",
    "        Alpha parameter for the Gamma prior on the precision (inverse variance) of the Gaussian prior.\n",
    "    b_parameter : float\n",
    "        Beta parameter for the Gamma prior on the precision (inverse variance) of the Gaussian prior.\n",
    "    k : int\n",
    "        Number of latent variables in the model.\n",
    "    em_iterations : int\n",
    "        Maximum number of iterations to run the EM algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[VariationalBayes, List[float]]\n",
    "        A tuple containing:\n",
    "        - The optimized VariationalBayes model.\n",
    "        - A list of free energy values recorded at each EM step, representing the convergence progress.\n",
    "    \"\"\"\n",
    "    # Calculate initial maximization parameters (means, variance, prior probabilities)\n",
    "    _, sigma, pi = VariationalBayes.calculate_maximisation_parameters(\n",
    "        x, binary_latent_factor_approximation\n",
    "    )\n",
    "\n",
    "    # Initialize Gaussian prior with provided hyperparameters\n",
    "    gaussian_prior = GaussianPrior(\n",
    "        a=a_parameter,\n",
    "        b=b_parameter,\n",
    "        d=x.shape[1],\n",
    "        k=k,\n",
    "    )\n",
    "\n",
    "    # Initialize the Variational Bayes model with the Gaussian prior, variance, and prior probabilities\n",
    "    variational_bayes_model = VariationalBayes(\n",
    "        mu=gaussian_prior,\n",
    "        variance=sigma**2,\n",
    "        pi=pi,\n",
    "    )\n",
    "\n",
    "    # Learn binary factors using the Variational Bayes model and update the free energy\n",
    "    _, variational_bayes_model, free_energy = learn_binary_factors(\n",
    "        x=x,\n",
    "        k=k,\n",
    "        em_iterations=em_iterations,\n",
    "        binary_latent_factor_model=variational_bayes_model,\n",
    "        binary_latent_factor_approximation=binary_latent_factor_approximation,\n",
    "    )\n",
    "\n",
    "    return variational_bayes_model, free_energy"
   ],
   "id": "ba27f0eeb7c09b67",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:58.986794Z",
     "start_time": "2024-12-24T14:48:58.979631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_converge(\n",
    "    free_energies: List[float],\n",
    "    current_lambda_matrix: np.ndarray,\n",
    "    previous_lambda_matrix: np.ndarray,\n",
    "    free_energy_threshold: float = 1e-6,\n",
    "    lambda_threshold: float = 1e-6,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether the algorithm has converged based on changes in free energy\n",
    "    and the lambda matrix.\n",
    "\n",
    "    Convergence is achieved if the change in free energy between the last two iterations\n",
    "    is below a specified threshold and the change in the lambda matrix (measured by\n",
    "    the Frobenius norm) is also below a specified threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    free_energies : List[float]\n",
    "        List of free energy values recorded at each iteration.\n",
    "    current_lambda_matrix : np.ndarray\n",
    "        The current lambda matrix after the latest iteration.\n",
    "    previous_lambda_matrix : np.ndarray\n",
    "        The lambda matrix from the previous iteration.\n",
    "    free_energy_threshold : float, optional\n",
    "        Threshold for the change in free energy to determine convergence, by default 1e-6.\n",
    "    lambda_threshold : float, optional\n",
    "        Threshold for the change in the lambda matrix (Frobenius norm) to determine convergence,\n",
    "        by default 1e-6.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if both the change in free energy and the change in lambda matrix are below\n",
    "        their respective thresholds, indicating convergence. Otherwise, False.\n",
    "    \"\"\"\n",
    "    if len(free_energies) < 2:\n",
    "        # Not enough data to determine convergence\n",
    "        return False\n",
    "\n",
    "    # Calculate the absolute change in free energy\n",
    "    free_energy_change = abs(free_energies[-1] - free_energies[-2])\n",
    "\n",
    "    # Calculate the Frobenius norm of the change in lambda matrix\n",
    "    lambda_change = np.linalg.norm(current_lambda_matrix - previous_lambda_matrix)\n",
    "\n",
    "    # Check if both changes are below their respective thresholds\n",
    "    return (free_energy_change <= free_energy_threshold) and (lambda_change <= lambda_threshold)\n",
    "\n",
    "\n",
    "def learn_binary_factors(\n",
    "    x: np.ndarray,\n",
    "    k: int,\n",
    "    em_iterations: int,\n",
    "    binary_latent_factor_model: 'VariationalBayes',\n",
    "    binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    ") -> Tuple['MeanFieldApproximation', 'VariationalBayes', List[float]]:\n",
    "    \"\"\"\n",
    "    Perform the Expectation-Maximization (EM) algorithm to learn binary latent factors.\n",
    "\n",
    "    This function iteratively performs the E-step and M-step to optimize the\n",
    "    variational approximation of binary latent factors and update the\n",
    "    variational Bayes model. It records the free energy at each iteration to\n",
    "    monitor convergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_dimensions), where n_samples is the\n",
    "        number of data points and n_dimensions is the number of observed dimensions.\n",
    "    em_iterations : int\n",
    "        Maximum number of EM iterations to perform.\n",
    "    binary_latent_factor_model : VariationalBayes\n",
    "        An instance of VariationalBayes representing the current model.\n",
    "    binary_latent_factor_approximation : MeanFieldApproximation\n",
    "        An instance of MeanFieldApproximation representing the current variational\n",
    "        approximation of the binary latent factors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[MeanFieldApproximation, VariationalBayes, List[float]]\n",
    "        A tuple containing:\n",
    "        - The updated MeanFieldApproximation instance.\n",
    "        - The updated VariationalBayes model.\n",
    "        - A list of free energy values recorded at each EM iteration.\n",
    "    \"\"\"\n",
    "    # Initialize the list of free energies with the initial free energy\n",
    "    free_energies: List[float] = [\n",
    "        binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for iteration in range(1, em_iterations + 1):\n",
    "        # Store the previous lambda matrix for convergence checking\n",
    "        previous_lambda_matrix = np.copy(binary_latent_factor_approximation.lambda_matrix)\n",
    "\n",
    "        # E-step: Update the variational approximation (lambda matrix)\n",
    "        free_energy_history = binary_latent_factor_approximation.variational_expectation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_model=binary_latent_factor_model,\n",
    "        )\n",
    "\n",
    "        # M-step: Update the variational Bayes model parameters\n",
    "        binary_latent_factor_model.maximisation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_approximation=binary_latent_factor_approximation,\n",
    "        )\n",
    "\n",
    "        # Compute and record the new free energy\n",
    "        current_free_energy = binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "        free_energies.append(current_free_energy)\n",
    "\n",
    "        # Check for convergence\n",
    "        if is_converge(\n",
    "            free_energies=free_energies,\n",
    "            current_lambda_matrix=binary_latent_factor_approximation.lambda_matrix,\n",
    "            previous_lambda_matrix=previous_lambda_matrix,\n",
    "        ):\n",
    "            print(f\"current K = {k},\"\n",
    "                  f\" Convergence achieved at iteration {iteration},\"\n",
    "                  f\" Free Energy at Convergence: {current_free_energy}.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return binary_latent_factor_approximation, binary_latent_factor_model, free_energies"
   ],
   "id": "99e1efdeeaf76289",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:59.010511Z",
     "start_time": "2024-12-24T14:48:58.997044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "\n",
    "def offset_image(coord: int, image_path: str, ax: Axes) -> None:\n",
    "    \"\"\"\n",
    "    Add an image to a specific coordinate on a Matplotlib axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : int\n",
    "        The x-coordinate on the axis where the image will be placed.\n",
    "    image_path : str\n",
    "        The file path to the image to be added.\n",
    "    ax : Axes\n",
    "        The Matplotlib axis to which the image will be added.\n",
    "    \"\"\"\n",
    "    img = plt.imread(image_path)\n",
    "    offset_img = OffsetImage(img, zoom=0.72)\n",
    "    offset_img.image.axes = ax\n",
    "\n",
    "    annotation = AnnotationBbox(\n",
    "        offset_img,\n",
    "        (coord, 0),\n",
    "        xybox=(0.0, -19.0),\n",
    "        frameon=False,\n",
    "        xycoords=\"data\",\n",
    "        boxcoords=\"offset points\",\n",
    "        pad=0,\n",
    "    )\n",
    "    ax.add_artist(annotation)\n",
    "\n",
    "\n",
    "def plot_factors(\n",
    "    variational_bayes_models: List['VariationalBayes'],\n",
    "    ks: List[int],\n",
    "    max_k: int,\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the latent factors and their corresponding inverse alpha values.\n",
    "\n",
    "    This function saves images of each latent factor and creates a bar plot\n",
    "    showing the inverse alpha values for each model. The first eight bars in\n",
    "    each bar plot are colored blue, and the remaining bars are grey.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    variational_bayes_models : List[VariationalBayes]\n",
    "        A list of VariationalBayes models corresponding to different numbers of latent factors.\n",
    "    ks : List[int]\n",
    "        A list of integers representing the number of latent factors for each model.\n",
    "    max_k : int\n",
    "        The maximum number of latent factors across all models.\n",
    "    save_path : str\n",
    "        The directory path where the plots and images will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Store each feature as an image for later use\n",
    "    for model_idx, k in enumerate(ks):\n",
    "        sorted_indices = np.argsort(\n",
    "            variational_bayes_models[model_idx].gaussian_prior.alpha\n",
    "        )\n",
    "        for factor_idx, alpha_idx in enumerate(sorted_indices):\n",
    "            fig = plt.figure(figsize=(0.3, 0.3))\n",
    "            ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            latent_factor_image = variational_bayes_models[model_idx].mu[:, alpha_idx].reshape(4, 4)\n",
    "            ax.imshow(latent_factor_image, cmap='gray')\n",
    "            image_filename = f\"-latent-factor-{model_idx}-{factor_idx}.png\"\n",
    "            fig.savefig(os.path.join(save_path, image_filename), bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "    # Create a bar plot of inverse alphas\n",
    "    fig, axes = plt.subplots(len(ks), 1, figsize=(15, 2 * len(ks)))\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    for model_idx, k in enumerate(ks):\n",
    "        sorted_indices = np.argsort(\n",
    "            variational_bayes_models[model_idx].gaussian_prior.alpha\n",
    "        )\n",
    "        inverse_alphas = (\n",
    "            1.0 / variational_bayes_models[model_idx].gaussian_prior.alpha[sorted_indices]\n",
    "        )\n",
    "        # Pad with zeros if necessary to match max_k\n",
    "        inverse_alphas_padded = list(inverse_alphas) + [0] * (max_k - k)\n",
    "        axes[model_idx].set_title(f\"Number of Latent Factors k = {k}\")\n",
    "        # Define colors: first eight bars blue, others grey\n",
    "        colors = ['blue' if idx < 8 else 'grey' for idx in range(max_k)]\n",
    "        axes[model_idx].bar(range(max_k), inverse_alphas_padded, color=colors)\n",
    "        axes[model_idx].set_xticks([])\n",
    "        axes[model_idx].set_ylabel(\"Inverse α\")\n",
    "\n",
    "    # Add feature image ticks\n",
    "    for model_idx, k in enumerate(ks):\n",
    "        sorted_indices = np.argsort(\n",
    "            variational_bayes_models[model_idx].gaussian_prior.alpha\n",
    "        )\n",
    "        for factor_idx in range(len(sorted_indices)):\n",
    "            image_path = os.path.join(save_path, f\"-latent-factor-{model_idx}-{factor_idx}.png\")\n",
    "            offset_image(factor_idx, image_path, axes[model_idx])\n",
    "            os.remove(image_path)\n",
    "\n",
    "    fig.savefig(save_path + f\"-latent-factors-comparison\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_free_energy(\n",
    "    ks: List[int],\n",
    "    free_energies: List[List[float]],\n",
    "    model_name: str,\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the free energy over EM iterations for different models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ks : List[int]\n",
    "        List of integers representing different numbers of latent factors.\n",
    "    free_energies : List[List[float]]\n",
    "        List containing lists of free energy values for each model.\n",
    "    model_name : str\n",
    "        Name of the model for the plot title.\n",
    "    save_path : str\n",
    "        Directory path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    shades = np.flip(np.linspace(0.3, 0.9, len(ks)))\n",
    "\n",
    "    for idx, k in enumerate(ks):\n",
    "        color = cmap(shades[idx])\n",
    "        ax.plot(free_energies[idx], label=f\"K = {k}\", color=color)\n",
    "\n",
    "    ax.set_title(f\"Free Energy of {model_name} Model Over Different Ks\")\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_ylabel(\"Free Energy\")\n",
    "    ax.legend()\n",
    "    plt.savefig(save_path + \"-free-energy\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ],
   "id": "452f89f296d2428f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:59.026769Z",
     "start_time": "2024-12-24T14:48:59.021060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def runARD(\n",
    "    x: np.ndarray,\n",
    "    a_parameter: int,\n",
    "    b_parameter: int,\n",
    "    ks: List[int],\n",
    "    max_k: int,\n",
    "    em_iterations: int,\n",
    "    e_maximum_steps: int,\n",
    "    e_convergence_criterion: float,\n",
    "    save_path: str,\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Execute Automatic Relevance Determination (ARD) across multiple models.\n",
    "\n",
    "    This function runs ARD for different numbers of latent factors, initializes the\n",
    "    corresponding mean field approximations, performs the Expectation-Maximization (EM)\n",
    "    algorithm to optimize the Variational Bayes models, collects free energy values\n",
    "    for each model, and plots the resulting latent factors along with their inverse\n",
    "    alpha values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_dimensions), where n_samples is the\n",
    "        number of data points and n_dimensions is the number of observed dimensions.\n",
    "    a_parameter : float\n",
    "        Alpha parameter for the Gamma prior on the precision (inverse variance) of the Gaussian prior.\n",
    "    b_parameter : float\n",
    "        Beta parameter for the Gamma prior on the precision (inverse variance) of the Gaussian prior.\n",
    "    ks : List[int]\n",
    "        List of integers representing different numbers of latent factors to evaluate.\n",
    "    max_k : int\n",
    "        The maximum number of latent factors across all models.\n",
    "    em_iterations : int\n",
    "        Number of iterations to run the EM algorithm.\n",
    "    e_maximum_steps : int\n",
    "        Maximum number of steps for the variational expectation step.\n",
    "    e_convergence_criterion : float\n",
    "        Convergence threshold for the variational expectation step.\n",
    "    save_path : str\n",
    "        Directory path where plots and images will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[float]]\n",
    "        A list containing lists of free energy values for each model.\n",
    "    \"\"\"\n",
    "    variational_bayes_models: List['VariationalBayes'] = []\n",
    "    free_energies: List[List[float]] = []\n",
    "\n",
    "    for model_idx, k in enumerate(ks):\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Initialize the MeanFieldApproximation instance\n",
    "        mean_field_approximation = init_mean_field_approximation(\n",
    "            k,\n",
    "            n,\n",
    "            max_steps=e_maximum_steps,\n",
    "            convergence_criterion=e_convergence_criterion,\n",
    "        )\n",
    "\n",
    "        # Run Automatic Relevance Determination (ARD)\n",
    "        variational_bayes_model, free_energy = automatic_relevance_determination(\n",
    "            x=x,\n",
    "            binary_latent_factor_approximation=mean_field_approximation,\n",
    "            a_parameter=a_parameter,\n",
    "            b_parameter=b_parameter,\n",
    "            k=k,\n",
    "            em_iterations=em_iterations,\n",
    "        )\n",
    "\n",
    "        variational_bayes_models.append(variational_bayes_model)\n",
    "        free_energies.append(free_energy)\n",
    "\n",
    "    # Plot the latent factors and inverse alpha values\n",
    "    plot_factors(\n",
    "        variational_bayes_models,\n",
    "        ks,\n",
    "        max_k,\n",
    "        save_path,\n",
    "    )\n",
    "\n",
    "    return free_energies"
   ],
   "id": "678c273dad433884",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:50:23.754518Z",
     "start_time": "2024-12-24T14:49:45.943786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Constants for output directories and random seed\n",
    "OUTPUTS_FOLDER = \"ARD\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform Automatic Relevance Determination (ARD) using Variational Bayes.\n",
    "\n",
    "    This function initializes the necessary directories, sets the random seed for reproducibility,\n",
    "    runs the ARD algorithm across different numbers of latent factors, and plots the free energy\n",
    "    progression for each model.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(DEFAULT_SEED)\n",
    "\n",
    "    # Ensure the main output directory exists\n",
    "    os.makedirs(OUTPUTS_FOLDER, exist_ok=True)\n",
    "\n",
    "    # Define output directory for Question 4\n",
    "\n",
    "\n",
    "    # Define parameters for ARD\n",
    "    a_parameter = 1.0\n",
    "    b_parameter = 0.0\n",
    "    latent_factors_list = list(range(4, 25))  # ks: number of latent factors from 4 to 24\n",
    "    max_latent_factors = 24\n",
    "    em_iterations = 100\n",
    "    expectation_maximization_steps = 100\n",
    "    expectation_convergence_threshold = 1e-6  # Changed from 0 to a small positive value for practical convergence\n",
    "\n",
    "    # Run Automatic Relevance Determination (ARD)\n",
    "    free_energies = runARD(\n",
    "        x=data,\n",
    "        a_parameter=a_parameter,\n",
    "        b_parameter=b_parameter,\n",
    "        ks=latent_factors_list,\n",
    "        max_k=max_latent_factors,\n",
    "        em_iterations=em_iterations,\n",
    "        e_maximum_steps=expectation_maximization_steps,\n",
    "        e_convergence_criterion=expectation_convergence_threshold,\n",
    "        save_path=os.path.join(OUTPUTS_FOLDER, \"b-1\"),\n",
    "    )\n",
    "\n",
    "    # Plot the free energy progression for each model\n",
    "    model_name = \"Variational Bayes\"\n",
    "    plot_free_energy(\n",
    "        ks=latent_factors_list,\n",
    "        free_energies=free_energies,\n",
    "        model_name=\"Variational Bayes\",\n",
    "        save_path=os.path.join(OUTPUTS_FOLDER, \"b-2\"),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "2c9b7793bf1e4374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current K = 4, Convergence achieved at iteration 41, Free Energy at Convergence: -9.502440567188636.\n",
      "current K = 5, Convergence achieved at iteration 54, Free Energy at Convergence: -8.189202403271139.\n",
      "current K = 6, Convergence achieved at iteration 38, Free Energy at Convergence: -6.884485244477967.\n",
      "current K = 7, Convergence achieved at iteration 48, Free Energy at Convergence: -5.563384475135205.\n",
      "current K = 8, Convergence achieved at iteration 14, Free Energy at Convergence: -5.8765661368619355.\n",
      "current K = 9, Convergence achieved at iteration 55, Free Energy at Convergence: -3.1628610534032613.\n",
      "current K = 10, Convergence achieved at iteration 38, Free Energy at Convergence: -1.1067765141713457.\n",
      "current K = 11, Convergence achieved at iteration 40, Free Energy at Convergence: -2.9764925575148737.\n",
      "current K = 12, Convergence achieved at iteration 41, Free Energy at Convergence: -0.15271753694523182.\n",
      "current K = 13, Convergence achieved at iteration 24, Free Energy at Convergence: 3.046545932150358.\n",
      "current K = 14, Convergence achieved at iteration 33, Free Energy at Convergence: 2.5300622483877833.\n",
      "current K = 15, Convergence achieved at iteration 54, Free Energy at Convergence: 3.4716289611198956.\n",
      "current K = 16, Convergence achieved at iteration 59, Free Energy at Convergence: 0.8414799805195926.\n",
      "current K = 17, Convergence achieved at iteration 58, Free Energy at Convergence: -2.9483299281069995.\n",
      "current K = 18, Convergence achieved at iteration 20, Free Energy at Convergence: 1.9597245770137606.\n",
      "current K = 19, Convergence achieved at iteration 20, Free Energy at Convergence: 0.42528427227330395.\n",
      "current K = 20, Convergence achieved at iteration 87, Free Energy at Convergence: -2.1279218354409895.\n",
      "current K = 21, Convergence achieved at iteration 40, Free Energy at Convergence: 0.081352000240101.\n",
      "current K = 23, Convergence achieved at iteration 30, Free Energy at Convergence: 1.415200494991428.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T14:48:59.216931Z",
     "start_time": "2024-12-24T14:22:35.548742Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ac40713e865b4beb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
