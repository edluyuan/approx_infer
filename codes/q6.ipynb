{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.318336Z",
     "start_time": "2024-12-26T19:05:08.308469Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "# ============================================================\n",
    "# Data Generation (This does the same thing as the genimages.py provided)\n",
    "# ============================================================\n",
    "np.random.seed(0)\n",
    "\n",
    "features = [\n",
    "    [0, 0, 1, 0,\n",
    "     0, 1, 1, 1,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [1, 0, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     1, 1, 0, 0,\n",
    "     1, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 1, 1, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1],\n",
    "]\n",
    "\n",
    "num_samples = 2000\n",
    "num_features = 16\n",
    "K_true = len(features)\n",
    "\n",
    "feature_weights = 0.5 + np.random.rand(K_true, 1) * 0.5\n",
    "mu_true = np.array([weight * feat for weight, feat in zip(feature_weights, features)])\n",
    "latent_factors = (np.random.rand(num_samples, K_true) < 0.3).astype(float)\n",
    "data = latent_factors @ mu_true + np.random.randn(num_samples, num_features)*0.1\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.366638Z",
     "start_time": "2024-12-26T19:05:08.360877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def m_step(X, ES, ESS):\n",
    "    \"\"\"\n",
    "    mu, sigma, pie = MStep(X,ES,ESS)\n",
    "\n",
    "    Inputs:\n",
    "    -----------------\n",
    "           X: shape (N, D) data matrix\n",
    "          ES: shape (N, K) E_q[s]\n",
    "         ESS: shape (K, K) sum over data points of E_q[ss'] (N, K, K)\n",
    "                           if E_q[ss'] is provided, the sum over N is done for you.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "          mu: shape (D, K) matrix of means in p(y|{s_i},mu,sigma)\n",
    "       sigma: shape (,)    standard deviation in same\n",
    "         pie: shape (1, K) vector of parameters specifying generative distribution for s\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    if ES.shape[0] != N:\n",
    "        raise TypeError('ES must have the same number of rows as X')\n",
    "    K = ES.shape[1]\n",
    "    if ESS.shape == (N, K, K):\n",
    "        ESS = np.sum(ESS, axis=0)\n",
    "    if ESS.shape != (K, K):\n",
    "        raise TypeError('ESS must be square and have the same number of columns as ES')\n",
    "\n",
    "    mu = np.dot(np.dot(np.linalg.inv(ESS), ES.T), X).T\n",
    "    sigma = np.sqrt((np.trace(np.dot(X.T, X)) + np.trace(np.dot(np.dot(mu.T, mu), ESS))\n",
    "                     - 2 * np.trace(np.dot(np.dot(ES.T, X), mu))) / (N * D))\n",
    "    pie = np.mean(ES, axis=0, keepdims=True)\n",
    "\n",
    "    return mu, sigma, pie\n"
   ],
   "id": "e5d5c1cecdc99240",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.415121Z",
     "start_time": "2024-12-26T19:05:08.390834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MessagePassing:\n",
    "    \"\"\"\n",
    "    Implements a message passing approximation for binary latent factor models using variational inference.\n",
    "\n",
    "    Attributes:\n",
    "        bernoulli_params (np.ndarray):\n",
    "            Bernoulli parameter tensor of shape (num_data_points, num_latents, num_latents).\n",
    "            - Off-diagonal entries represent \\tilde{g}_{ij, \\neg s_i}(s_j) for data point n.\n",
    "            - Diagonal entries represent \\tilde{f}_i(s_i).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bernoulli_parameter_matrix: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the MessagePassing with a given Bernoulli parameter tensor.\n",
    "\n",
    "        Args:\n",
    "            bernoulli_parameter_matrix (np.ndarray):\n",
    "                Initial Bernoulli parameter tensor with shape\n",
    "                (num_data_points, num_latents, num_latents).\n",
    "        \"\"\"\n",
    "        self.bernoulli_parameter_matrix = bernoulli_parameter_matrix\n",
    "\n",
    "    @property\n",
    "    def expectation_s(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the expectation of the latent variables S.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Lambda matrix representing E[S].\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix\n",
    "\n",
    "    @property\n",
    "    def expectation_ss(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the expectation of the outer product of latent variables S.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: E[SS^T].\n",
    "        \"\"\"\n",
    "        ess = self.lambda_matrix.T @ self.lambda_matrix\n",
    "        # Replace diagonal with correct E[s_i^2] = E[s_i] = lambda_i\n",
    "        np.fill_diagonal(ess, self.lambda_matrix.sum(axis=0))\n",
    "        return ess\n",
    "\n",
    "    @property\n",
    "    def log_lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the logarithm of the lambda matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: log(E[S]).\n",
    "        \"\"\"\n",
    "        return np.log(self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the logarithm of (1 - lambda matrix).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: log(1 - E[S]).\n",
    "        \"\"\"\n",
    "        return np.log(1 - self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of data points.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of data points (n).\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of latent variables.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of latent variables (k).\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[1]\n",
    "\n",
    "    def compute_free_energy(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: 'LoopyBP',\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Computes the free energy associated with the current EM parameters and data.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            binary_latent_factor_model (LoopyBP): Binary latent factor model instance.\n",
    "\n",
    "        Returns:\n",
    "            float: Average free energy per data point.\n",
    "        \"\"\"\n",
    "        expectation_log_p_x_s_given_theta = (\n",
    "            self._compute_expectation_log_p_x_s_given_theta(\n",
    "                x, binary_latent_factor_model\n",
    "            )\n",
    "        )\n",
    "        entropy = self._compute_approximation_model_entropy()\n",
    "        free_energy = (expectation_log_p_x_s_given_theta + entropy) / self.n\n",
    "        return free_energy\n",
    "\n",
    "    def _compute_expectation_log_p_x_s_given_theta(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: \"LoopyBP\",\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Computes the expectation of log P(X, S | theta).\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            binary_latent_factor_model (LoopyBP): Binary latent factor model instance.\n",
    "\n",
    "        Returns:\n",
    "            float: Expectation of log P(X, S | theta).\n",
    "        \"\"\"\n",
    "        # Compute expected mean: E[S] @ mu^T\n",
    "        expected_mean = self.lambda_matrix @ binary_latent_factor_model.mu.T  # Shape: (n, d)\n",
    "\n",
    "        # Compute E[S S^T] element-wise multiplied by mu mu^T\n",
    "        expected_ss_mu_mu = np.multiply(\n",
    "            self.expectation_ss,\n",
    "            binary_latent_factor_model.mu.T @ binary_latent_factor_model.mu\n",
    "        )  # Shape: (k, k)\n",
    "\n",
    "        # Compute the expectation of log P(X | S, theta)\n",
    "        term1 = - (self.n * binary_latent_factor_model.k / 2) * np.log(2 * np.pi * binary_latent_factor_model.variance)\n",
    "        term2 = -0.5 * binary_latent_factor_model.precision * (\n",
    "            np.sum(x ** 2)\n",
    "            - 2 * np.sum(x * expected_mean)\n",
    "            + np.sum(expected_ss_mu_mu)\n",
    "            - np.trace(expected_ss_mu_mu)  # Remove E[s_i^2] and add correct E[s_i]\n",
    "            + np.sum(\n",
    "                self.lambda_matrix @ (binary_latent_factor_model.mu ** 2).T\n",
    "            )\n",
    "        )\n",
    "        expectation_log_p_x_given_s_theta = term1 + term2\n",
    "\n",
    "        # Compute the expectation of log P(S | theta)\n",
    "        expectation_log_p_s_given_theta = np.sum(\n",
    "            self.lambda_matrix * binary_latent_factor_model.log_pi\n",
    "            + (1 - self.lambda_matrix) * binary_latent_factor_model.log_one_minus_pi\n",
    "        )\n",
    "\n",
    "        return expectation_log_p_x_given_s_theta + expectation_log_p_s_given_theta\n",
    "\n",
    "\n",
    "    def _compute_approximation_model_entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        Computes the entropy of the approximation model.\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy.\n",
    "        \"\"\"\n",
    "\n",
    "        entropy = -np.sum(\n",
    "            self.lambda_matrix * self.log_lambda_matrix\n",
    "            + (1 - self.lambda_matrix) * self.log_one_minus_lambda_matrix\n",
    "        )\n",
    "        return entropy\n",
    "\n",
    "    @property\n",
    "    def lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the lambda matrix by aggregating natural parameters and applying the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Lambda matrix representing E[S].\n",
    "        \"\"\"\n",
    "        aggregated_natural_params = self.natural_parameter_matrix.sum(axis=1)  # Sum over incoming messages\n",
    "        lambda_matrix = 1 / (1 + np.exp(-aggregated_natural_params))\n",
    "        # Numerical stability\n",
    "        lambda_matrix = np.clip(lambda_matrix, 1e-10, 1 - 1e-10)\n",
    "        return lambda_matrix\n",
    "\n",
    "    @property\n",
    "    def natural_parameter_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the natural parameters (eta) from the Bernoulli parameters.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Natural parameter matrix.\n",
    "        \"\"\"\n",
    "        odds = self.bernoulli_parameter_matrix / (1 - self.bernoulli_parameter_matrix)\n",
    "        natural_params = np.log(odds)\n",
    "        return natural_params\n",
    "\n",
    "    def aggregate_incoming_binary_factor_messages(\n",
    "        self, node_index: int, excluded_node_index: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Aggregates incoming natural messages to a target node, excluding messages from a specific node.\n",
    "\n",
    "        Args:\n",
    "            target_node (int): Index of the target latent variable.\n",
    "            exclude_node (int): Index of the node to exclude from aggregation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Aggregated natural parameters for each data point.\n",
    "        \"\"\"\n",
    "        # Sum natural parameters from all nodes except the excluded node\n",
    "        incoming_before = self.natural_parameter_matrix[:, :excluded_node_index, node_index]\n",
    "        incoming_after = self.natural_parameter_matrix[:, excluded_node_index + 1 :, node_index]\n",
    "        aggregated = np.sum(incoming_before, axis=1) + np.sum(incoming_after, axis=1)\n",
    "        return aggregated.reshape(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_bernoulli_parameter(\n",
    "            natural_parameter_matrix: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes Bernoulli parameters from natural parameters using the sigmoid function.\n",
    "\n",
    "        Args:\n",
    "            natural_params (np.ndarray): Natural parameter matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Updated Bernoulli parameters.\n",
    "        \"\"\"\n",
    "        bernoulli_parameter = 1 / (1 + np.exp(-natural_parameter_matrix))\n",
    "        # Numerical stability\n",
    "        bernoulli_parameter = np.clip(bernoulli_parameter, 1e-10, 1 - 1e-10)\n",
    "        return bernoulli_parameter\n",
    "\n",
    "    def variational_expectation_step(\n",
    "        self, x: np.ndarray, binary_latent_factor_model: 'LoopyBP'\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Performs the variational expectation step, updating singleton and binary factors iteratively.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            binary_latent_factor_model (LoopyBP): Binary latent factor model instance.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of free energies after each update.\n",
    "        \"\"\"\n",
    "        free_energies = [self.compute_free_energy(x, binary_latent_factor_model)]\n",
    "\n",
    "        for i in range(self.k):\n",
    "            # Update singleton factor for latent variable i\n",
    "            singleton_natural = self.calculate_singleton_message_update(\n",
    "                LoopyBP=binary_latent_factor_model,\n",
    "                x=x,\n",
    "                i=i,\n",
    "            )\n",
    "            self.bernoulli_parameter_matrix[:, i, i] = self.calculate_bernoulli_parameter(singleton_natural)\n",
    "            free_energies.append(self.compute_free_energy(x, binary_latent_factor_model))\n",
    "\n",
    "            # Update binary factors between latent variables i and j\n",
    "            for j in range(i):\n",
    "                # Update message from i to j\n",
    "                binary_natural_ij = self.calculate_binary_message_update(\n",
    "                    LoopyBP=binary_latent_factor_model,\n",
    "                    x=x,\n",
    "                    i=i,\n",
    "                    j=j,\n",
    "                )\n",
    "                self.bernoulli_parameter_matrix[:, i, j] = self.calculate_bernoulli_parameter(binary_natural_ij)\n",
    "\n",
    "                # Update message from j to i\n",
    "                binary_natural_ji = self.calculate_binary_message_update(\n",
    "                    LoopyBP=binary_latent_factor_model,\n",
    "                    x=x,\n",
    "                    i=i,\n",
    "                    j=j,\n",
    "                )\n",
    "                self.bernoulli_parameter_matrix[:, j, i] = self.calculate_bernoulli_parameter(binary_natural_ji)\n",
    "\n",
    "                free_energies.append(self.compute_free_energy(x, binary_latent_factor_model))\n",
    "\n",
    "        return free_energies\n",
    "\n",
    "\n",
    "    def calculate_binary_message_update(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        LoopyBP: \"LoopyBP\",\n",
    "        i: int,\n",
    "        j: int,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Updates the natural parameters for a binary factor between two latent variables.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            latent_model (LoopyBP): Binary latent factor model instance.\n",
    "            i, source (int): Index of the source latent variable.\n",
    "            j, target (int): Index of the target latent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Updated natural parameters for the binary factor.\n",
    "        \"\"\"\n",
    "        # Compute natural parameters excluding the target node\n",
    "        natural_parameter_i_not_j = LoopyBP.b_index(x=x, node_index=i)\n",
    "        natural_parameter_i_not_j += self.aggregate_incoming_binary_factor_messages(node_index=i, excluded_node_index=j)\n",
    "\n",
    "        # Retrieve interaction weight between source and target\n",
    "        w_i_j = LoopyBP.w_matrix_index(i, j)\n",
    "\n",
    "        # Update natural parameters for the binary factor\n",
    "        updated_natural = np.log1p(np.exp(w_i_j + natural_parameter_i_not_j)) - np.log1p(np.exp(natural_parameter_i_not_j))\n",
    "        return updated_natural\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_singleton_message_update(\n",
    "        x: np.ndarray,\n",
    "        LoopyBP: \"LoopyBP\",\n",
    "        i: int,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Updates the natural parameters for a singleton factor of a latent variable.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            latent_model (LoopyBP): Binary latent factor model instance.\n",
    "            i, latent_idx (int): Index of the latent variable to update.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Updated natural parameters for the singleton factor.\n",
    "        \"\"\"\n",
    "        # Singleton update does not require message aggregation\n",
    "        return LoopyBP.b_index(x=x, node_index=i)\n",
    "\n",
    "\n",
    "def init_message_passing(k: int, n: int) -> MessagePassing:\n",
    "    \"\"\"\n",
    "    Message passing initialisation\n",
    "\n",
    "    :param k: number of latent variables\n",
    "    :param n: number of data points\n",
    "    :return: message passing\n",
    "    \"\"\"\n",
    "    bernoulli_parameter_matrix = np.random.random(size=(n, k, k))\n",
    "    return MessagePassing(bernoulli_parameter_matrix)\n"
   ],
   "id": "c255d7fcf66531db",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.433723Z",
     "start_time": "2024-12-26T19:05:08.419484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LoopyBP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu: np.ndarray,\n",
    "        sigma: float,\n",
    "        pi: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the LoopyBP model with mean, variance, and prior probabilities.\n",
    "\n",
    "        Args:\n",
    "            mu (np.ndarray): Mean matrix of shape (num_dimensions, num_latents).\n",
    "            sigma (float): Standard deviation parameter.\n",
    "            pi (np.ndarray): Prior probabilities of latent variables, shape (1, num_latents).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._mu = mu  # Shape: (num_dimensions, num_latents)\n",
    "        self._sigma = sigma\n",
    "        self._pi = pi   # Shape: (1, num_latents)\n",
    "    # ==========================\n",
    "    # Property Definitions\n",
    "    # ==========================\n",
    "\n",
    "    @property\n",
    "    def d(self) -> int:\n",
    "        return self.mu.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self.mu.shape[1]\n",
    "\n",
    "    @property\n",
    "    def mu(self) -> np.ndarray:\n",
    "        \"\"\"Mean matrix.\"\"\"\n",
    "        return self._mu\n",
    "\n",
    "    @mu.setter\n",
    "    def mu(self, value: np.ndarray) -> None:\n",
    "        self._mu = value\n",
    "\n",
    "    @property\n",
    "    def sigma(self) -> float:\n",
    "        \"\"\"Standard deviation.\"\"\"\n",
    "        return self._sigma\n",
    "\n",
    "    @sigma.setter\n",
    "    def sigma(self, value: float) -> None:\n",
    "        self._sigma = value\n",
    "\n",
    "    @property\n",
    "    def pi(self) -> np.ndarray:\n",
    "        \"\"\"Prior probabilities of latent variables.\"\"\"\n",
    "        return self._pi\n",
    "\n",
    "    @pi.setter\n",
    "    def pi(self, value: np.ndarray) -> None:\n",
    "        self._pi = value\n",
    "\n",
    "    @property\n",
    "    def variance(self) -> float:\n",
    "        \"\"\"Variance, square of the standard deviation.\"\"\"\n",
    "        return self.sigma ** 2\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        \"\"\"Precision, inverse of variance.\"\"\"\n",
    "        return 1.0 / self.variance\n",
    "\n",
    "    @property\n",
    "    def log_pi(self) -> np.ndarray:\n",
    "        \"\"\"Logarithm of prior probabilities.\"\"\"\n",
    "        return np.log(self.pi)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_pi(self) -> np.ndarray:\n",
    "        \"\"\"Logarithm of (1 - prior probabilities).\"\"\"\n",
    "        return np.log(1 - self.pi)\n",
    "\n",
    "    @property\n",
    "    def log_pi_ratio(self) -> np.ndarray:\n",
    "        \"\"\"Log ratio of pi to (1 - pi).\"\"\"\n",
    "        return self.log_pi - self.log_one_minus_pi\n",
    "\n",
    "    @property\n",
    "    def num_dimensions(self) -> int:\n",
    "        \"\"\"Number of dimensions in the data.\"\"\"\n",
    "        return self.mu.shape[0]\n",
    "\n",
    "    @property\n",
    "    def num_latents(self) -> int:\n",
    "        \"\"\"Number of latent variables.\"\"\"\n",
    "        return self.mu.shape[1]\n",
    "\n",
    "    # ==========================\n",
    "    # Static Methods\n",
    "    # ==========================\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_maximisation_parameters(\n",
    "        x: np.ndarray,\n",
    "        approximation: MessagePassing,\n",
    "    ) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Performs the Maximization (M) step to update model parameters based on expectations.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            approximation (MessagePassing): Variational approximation instance.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float, np.ndarray]:\n",
    "                - Updated mu matrix.\n",
    "                - Updated sigma (float).\n",
    "                - Updated pi vector.\n",
    "        \"\"\"\n",
    "        return m_step(\n",
    "            X=x,\n",
    "            ES=approximation.expectation_s,\n",
    "            ESS=approximation.expectation_ss,\n",
    "        )\n",
    "\n",
    "    # ==========================\n",
    "    # Instance Methods\n",
    "    # ==========================\n",
    "\n",
    "    def maximisation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: MessagePassing,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the model parameters by performing the Maximization step.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            binary_latent_factor_approximation (MessagePassing): Variational approximation instance.\n",
    "        \"\"\"\n",
    "        mu_updated, sigma_updated, pi_updated = self.calculate_maximisation_parameters(\n",
    "            x, binary_latent_factor_approximation\n",
    "        )\n",
    "        self.mu = mu_updated\n",
    "        self.sigma = sigma_updated\n",
    "        self.pi = pi_updated\n",
    "\n",
    "    def w_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the weight matrix for Loopy Belief Propagation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Weight matrix of shape (num_latents, num_latents).\n",
    "        \"\"\"\n",
    "        return -self.precision * (self.mu.T @ self.mu)\n",
    "\n",
    "    def w_matrix_index(self, i, j) -> float:\n",
    "        \"\"\"\n",
    "        Retrieves the weight between two specific latent variables.\n",
    "\n",
    "        Args:\n",
    "            source (int): Index of the source latent variable.\n",
    "            target (int): Index of the target latent variable.\n",
    "\n",
    "        Returns:\n",
    "            float: Weight value between the specified latent variables.\n",
    "        \"\"\"\n",
    "        return -self.precision * (self.mu[:, i] @ self.mu[:, j])\n",
    "\n",
    "    def b(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the 'b' term in LoopyBP for all data points.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Computed 'b' matrix of shape (num_data_points, num_latents).\n",
    "        \"\"\"\n",
    "        return -(\n",
    "            self.precision * x @ self.mu\n",
    "            + self.log_pi_ratio\n",
    "            - 0.5 * self.precision * np.sum(self.mu ** 2, axis=0)\n",
    "        )\n",
    "\n",
    "    def b_index(self, x: np.ndarray, node_index: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the 'b' term for a specific node in LoopyBP across all data points.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "            node_index (int): Index of the target latent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Computed 'b' vector for the specified node, shape (num_data_points,).\n",
    "        \"\"\"\n",
    "        return -(\n",
    "            self.precision * x @ self.mu[:, node_index]\n",
    "            + (self.log_pi[0, node_index] - self.log_one_minus_pi[0, node_index])\n",
    "            - 0.5 * self.precision * (self.mu[:, node_index] @ self.mu[:, node_index])\n",
    "        ).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_LoopyBP(\n",
    "    x: np.ndarray,\n",
    "    binary_latent_factor_approximation: \"MessagePassing\",\n",
    ") -> LoopyBP:\n",
    "    \"\"\"\n",
    "    Initialise the Loopy BP model by running a single m step with the parameters of a given binary latent factor approximation\n",
    "    \"\"\"\n",
    "    mu, sigma, pi = LoopyBP.calculate_maximisation_parameters(\n",
    "        x, binary_latent_factor_approximation\n",
    "    )\n",
    "    return LoopyBP(mu=mu, sigma=sigma, pi=pi)\n"
   ],
   "id": "ae1bd2432c12f4b9",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.452234Z",
     "start_time": "2024-12-26T19:05:08.445374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_latent_features(\n",
    "    mu: np.ndarray,\n",
    "    num_latents: int,\n",
    "    feature_shape: Tuple[int, int],\n",
    "    title: str,\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the latent features as images.\n",
    "\n",
    "    Args:\n",
    "        mu (np.ndarray): Mean matrix of shape (num_dimensions, num_latents).\n",
    "        num_latents (int): Number of latent variables (factors).\n",
    "        feature_shape (Tuple[int, int]): Shape to which each latent factor is reshaped for visualization.\n",
    "        title (str): Title of the plot.\n",
    "        save_path (str): File path to save the plot.\n",
    "    \"\"\"\n",
    "    # Handle the case where num_latents might be 1\n",
    "    if num_latents == 1:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(2, 2))\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, num_latents, figsize=(num_latents * 2, 2))\n",
    "\n",
    "    for i in range(num_latents):\n",
    "        # Reshape the latent factor for visualization\n",
    "        try:\n",
    "            feature_image = mu[:, i].reshape(feature_shape)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\n",
    "                f\"Cannot reshape latent factor {i} to shape {feature_shape}: {e}\"\n",
    "            )\n",
    "\n",
    "        axes[i].imshow(feature_image, cmap='gray', interpolation='none')\n",
    "        axes[i].set_title(f\"Feature {i + 1}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_free_energy(\n",
    "    free_energy: List[float],\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the free energy over EM iterations.\n",
    "\n",
    "    Args:\n",
    "        free_energy (List[float]): List of free energy values over iterations.\n",
    "        title (str): Title of the plot.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        save_path (str): File path to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(free_energy, marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n"
   ],
   "id": "abab2f614e06d070",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.478710Z",
     "start_time": "2024-12-26T19:05:08.473282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def runLoopyBP(\n",
    "        x: np.ndarray,\n",
    "        k: int,\n",
    "        em_iterations: int,\n",
    "        save_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Executes the Loopy Belief Propagation (LoopyBP) algorithm for binary latent factor models,\n",
    "    including initialization, expectation-maximization (EM) iterations, and visualization\n",
    "    of latent features and free energy over iterations.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Data matrix of shape (num_data_points, num_dimensions).\n",
    "        k (int): Number of latent variables (factors) in the model.\n",
    "        em_iterations (int): Number of EM iterations to perform.\n",
    "        save_path (str): Base path for saving generated plots.\n",
    "\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Initialize message passing and LoopyBP models\n",
    "    message_passing = init_message_passing(k, n)\n",
    "    LoopyBP = init_LoopyBP(x, message_passing)\n",
    "\n",
    "    # Plot and save initial latent features\n",
    "    plot_latent_features(\n",
    "        mu=LoopyBP.mu,\n",
    "        num_latents=k,\n",
    "        feature_shape=(4, 4),\n",
    "        title=\"Initial Latent Features (Loopy BP)\",\n",
    "        save_path=f\"{save_path}-init-latent-factors.png\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Perform EM Updates\n",
    "    message_passing, LoopyBP, free_energy = learn_binary_factors(\n",
    "        x=x,\n",
    "        k=k,\n",
    "        em_iterations=em_iterations,\n",
    "        binary_latent_factor_model=LoopyBP,\n",
    "        binary_latent_factor_approximation=message_passing,\n",
    "    )\n",
    "\n",
    "    # Plot and save learned latent features\n",
    "    plot_latent_features(\n",
    "        mu=LoopyBP.mu,\n",
    "        num_latents=k,\n",
    "        feature_shape=(4, 4),\n",
    "        title=\"Learned Latent Features (Loopy BP)\",\n",
    "        save_path=f\"{save_path}-learned-latent-factors.png\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Plot and save free energy over EM iterations\n",
    "    plot_free_energy(\n",
    "        free_energy=free_energy,\n",
    "        title=\"Free Energy (Loopy BP)\",\n",
    "        xlabel=\"Iterations\",\n",
    "        ylabel=\"Free Energy\",\n",
    "        save_path=f\"{save_path}-free-energy.png\",\n",
    "    )\n",
    "\n"
   ],
   "id": "5cd28f1b8c99c691",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:05:08.507849Z",
     "start_time": "2024-12-26T19:05:08.499671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_converge(\n",
    "    free_energies: List[float],\n",
    "    current_lambda_matrix: np.ndarray,\n",
    "    previous_lambda_matrix: np.ndarray,\n",
    "    free_energy_threshold: float = 1e-6,\n",
    "    lambda_threshold: float = 1e-6,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether the algorithm has converged based on changes in free energy\n",
    "    and the lambda matrix.\n",
    "\n",
    "    Convergence is achieved if the change in free energy between the last two iterations\n",
    "    is below a specified threshold and the change in the lambda matrix (measured by\n",
    "    the Frobenius norm) is also below a specified threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    free_energies : List[float]\n",
    "        List of free energy values recorded at each iteration.\n",
    "    current_lambda_matrix : np.ndarray\n",
    "        The current lambda matrix after the latest iteration.\n",
    "    previous_lambda_matrix : np.ndarray\n",
    "        The lambda matrix from the previous iteration.\n",
    "    free_energy_threshold : float, optional\n",
    "        Threshold for the change in free energy to determine convergence, by default 1e-6.\n",
    "    lambda_threshold : float, optional\n",
    "        Threshold for the change in the lambda matrix (Frobenius norm) to determine convergence,\n",
    "        by default 1e-6.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if both the change in free energy and the change in lambda matrix are below\n",
    "        their respective thresholds, indicating convergence. Otherwise, False.\n",
    "    \"\"\"\n",
    "    if len(free_energies) < 2:\n",
    "        # Not enough data to determine convergence\n",
    "        return False\n",
    "\n",
    "    # Calculate the absolute change in free energy\n",
    "    free_energy_change = abs(free_energies[-1] - free_energies[-2])\n",
    "\n",
    "    # Calculate the Frobenius norm of the change in lambda matrix\n",
    "    lambda_change = np.linalg.norm(current_lambda_matrix - previous_lambda_matrix)\n",
    "\n",
    "    # Check if both changes are below their respective thresholds\n",
    "    return (free_energy_change <= free_energy_threshold) and (lambda_change <= lambda_threshold)\n",
    "\n",
    "\n",
    "def learn_binary_factors(\n",
    "    x: np.ndarray,\n",
    "    k: int,\n",
    "    em_iterations: int,\n",
    "    binary_latent_factor_model: 'LoopyBP',\n",
    "    binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    ") -> Tuple['MeanFieldApproximation', 'LoopyBP', List[float]]:\n",
    "    \"\"\"\n",
    "    Perform the Expectation-Maximization (EM) algorithm to learn binary latent factors.\n",
    "\n",
    "    This function iteratively performs the E-step and M-step to optimize the\n",
    "    variational approximation of binary latent factors and update the\n",
    "    variational Bayes model. It records the free energy at each iteration to\n",
    "    monitor convergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_dimensions), where n_samples is the\n",
    "        number of data points and n_dimensions is the number of observed dimensions.\n",
    "    em_iterations : int\n",
    "        Maximum number of EM iterations to perform.\n",
    "    binary_latent_factor_model : VariationalBayes\n",
    "        An instance of VariationalBayes representing the current model.\n",
    "    binary_latent_factor_approximation : MeanFieldApproximation\n",
    "        An instance of MeanFieldApproximation representing the current variational\n",
    "        approximation of the binary latent factors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[MeanFieldApproximation, VariationalBayes, List[float]]\n",
    "        A tuple containing:\n",
    "        - The updated MeanFieldApproximation instance.\n",
    "        - The updated VariationalBayes model.\n",
    "        - A list of free energy values recorded at each EM iteration.\n",
    "    \"\"\"\n",
    "    # Initialize the list of free energies with the initial free energy\n",
    "    free_energies: List[float] = [\n",
    "        binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for iteration in range(1, em_iterations + 1):\n",
    "        # Store the previous lambda matrix for convergence checking\n",
    "        previous_lambda_matrix = np.copy(binary_latent_factor_approximation.lambda_matrix)\n",
    "\n",
    "        # E-step: Update the variational approximation (lambda matrix)\n",
    "        free_energy_history = binary_latent_factor_approximation.variational_expectation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_model=binary_latent_factor_model,\n",
    "        )\n",
    "\n",
    "        # M-step: Update the variational Bayes model parameters\n",
    "        binary_latent_factor_model.maximisation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_approximation=binary_latent_factor_approximation,\n",
    "        )\n",
    "\n",
    "        # Compute and record the new free energy\n",
    "        current_free_energy = binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "        free_energies.append(current_free_energy)\n",
    "\n",
    "        # Check for convergence\n",
    "        if is_converge(\n",
    "            free_energies=free_energies,\n",
    "            current_lambda_matrix=binary_latent_factor_approximation.lambda_matrix,\n",
    "            previous_lambda_matrix=previous_lambda_matrix,\n",
    "        ):\n",
    "            print(f\"current K = {k},\"\n",
    "                  f\" Convergence achieved at iteration {iteration},\"\n",
    "                  f\" Free Energy at Convergence: {current_free_energy}.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return binary_latent_factor_approximation, binary_latent_factor_model, free_energies"
   ],
   "id": "e8d4b00a4765abb2",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-26T19:05:08.528521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Constants for output directories and random seed\n",
    "OUTPUTS_FOLDER = \"LoopyBP\"\n",
    "DEFAULT_SEED = 43\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(DEFAULT_SEED)\n",
    "\n",
    "    if not os.path.exists(OUTPUTS_FOLDER):\n",
    "        os.makedirs(OUTPUTS_FOLDER)\n",
    "\n",
    "\n",
    "    x = data\n",
    "    k = 8\n",
    "    em_iterations = 100\n",
    "    e_maximum_steps = 50\n",
    "    e_convergence_criterion = 0\n",
    "\n",
    "    runLoopyBP(x, k, em_iterations, save_path=os.path.join(OUTPUTS_FOLDER, \"all\"))"
   ],
   "id": "44a08f38794c2f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T18:27:44.103424Z",
     "start_time": "2024-12-26T18:27:44.101304Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ef9b259bcdea6328",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
