{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:09.999445Z",
     "start_time": "2024-12-25T17:02:09.558671Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "# ============================================================\n",
    "# Data Generation (This does the same thing as the genimages.py provided)\n",
    "# ============================================================\n",
    "np.random.seed(0)\n",
    "\n",
    "features = [\n",
    "    [0, 0, 1, 0,\n",
    "     0, 1, 1, 1,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [1, 0, 0, 0,\n",
    "     0, 1, 0, 0,\n",
    "     0, 0, 1, 0,\n",
    "     0, 0, 0, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 0, 0, 0,\n",
    "     1, 1, 0, 0,\n",
    "     1, 1, 0, 0],\n",
    "    [1, 1, 1, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 0, 0, 1,\n",
    "     1, 1, 1, 1],\n",
    "    [0, 0, 0, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 1, 1, 0,\n",
    "     0, 0, 0, 0],\n",
    "    [0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1,\n",
    "     0, 0, 0, 1],\n",
    "]\n",
    "\n",
    "num_samples = 2000\n",
    "num_features = 16\n",
    "K_true = len(features)\n",
    "\n",
    "feature_weights = 0.5 + np.random.rand(K_true, 1) * 0.5\n",
    "mu_true = np.array([weight * feat for weight, feat in zip(feature_weights, features)])\n",
    "latent_factors = (np.random.rand(num_samples, K_true) < 0.3).astype(float)\n",
    "data = latent_factors @ mu_true + np.random.randn(num_samples, num_features)*0.1\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:10.015581Z",
     "start_time": "2024-12-25T17:02:10.004357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import TYPE_CHECKING, List\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractBinaryLatentFactorApproximation(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        lambda_matrix: parameters variational approximation (number_of_points, number_of_latent_variables)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def variational_expectation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: AbstractBinaryLatentFactorModel,\n",
    "    ) -> List[float]:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def expectation_s(self):\n",
    "        return self.lambda_matrix\n",
    "\n",
    "    @property\n",
    "    def expectation_ss(self):\n",
    "        ess = self.lambda_matrix.T @ self.lambda_matrix\n",
    "        np.fill_diagonal(ess, self.lambda_matrix.sum(axis=0))\n",
    "        return ess\n",
    "\n",
    "    @property\n",
    "    def log_lambda_matrix(self) -> np.ndarray:\n",
    "        return np.log(self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_lambda_matrix(self) -> np.ndarray:\n",
    "        return np.log(1 - self.lambda_matrix)\n",
    "\n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of data points\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of latent variables\n",
    "        \"\"\"\n",
    "        return self.lambda_matrix.shape[1]\n",
    "\n",
    "    def compute_free_energy(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: AbstractBinaryLatentFactorModel,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        free energy associated with current EM parameters and data x\n",
    "\n",
    "        :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "        :param binary_latent_factor_model: a binary_latent_factor_model\n",
    "        :return: average free energy per data point\n",
    "        \"\"\"\n",
    "        expectation_log_p_x_s_given_theta = (\n",
    "            self._compute_expectation_log_p_x_s_given_theta(\n",
    "                x, binary_latent_factor_model\n",
    "            )\n",
    "        )\n",
    "        approximation_model_entropy = self._compute_approximation_model_entropy()\n",
    "        return (\n",
    "            expectation_log_p_x_s_given_theta + approximation_model_entropy\n",
    "        ) / self.n\n",
    "\n",
    "    def _compute_expectation_log_p_x_s_given_theta(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_model: AbstractBinaryLatentFactorModel,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        The first term of the free energy, the expectation of log P(X,S|theta)\n",
    "\n",
    "        :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "        :param binary_latent_factor_model: a binary_latent_factor_model\n",
    "        :return: the expectation of log P(X,S|theta)\n",
    "        \"\"\"\n",
    "        # (number_of_points, number_of_dimensions)\n",
    "        mu_lambda = self.lambda_matrix @ binary_latent_factor_model.mu.T\n",
    "\n",
    "        # (number_of_latent_variables, number_of_latent_variables)\n",
    "        expectation_s_i_s_j_mu_i_mu_j = np.multiply(\n",
    "            self.lambda_matrix.T @ self.lambda_matrix,\n",
    "            binary_latent_factor_model.mu.T @ binary_latent_factor_model.mu,\n",
    "        )\n",
    "\n",
    "        expectation_log_p_x_given_s_theta = -(\n",
    "            self.n * binary_latent_factor_model.d / 2\n",
    "        ) * np.log(2 * np.pi * binary_latent_factor_model.variance) - (\n",
    "            0.5 * binary_latent_factor_model.precision\n",
    "        ) * (\n",
    "            np.sum(np.multiply(x, x))\n",
    "            - 2 * np.sum(np.multiply(x, mu_lambda))\n",
    "            + np.sum(expectation_s_i_s_j_mu_i_mu_j)\n",
    "            - np.trace(\n",
    "                expectation_s_i_s_j_mu_i_mu_j\n",
    "            )  # remove incorrect E[s_i s_i] = lambda_i * lambda_i\n",
    "            + np.sum(  # add correct E[s_i s_i] = lambda_i\n",
    "                self.lambda_matrix\n",
    "                @ np.multiply(\n",
    "                    binary_latent_factor_model.mu, binary_latent_factor_model.mu\n",
    "                ).T\n",
    "            )\n",
    "        )\n",
    "        expectation_log_p_s_given_theta = np.sum(\n",
    "            np.multiply(\n",
    "                self.lambda_matrix,\n",
    "                binary_latent_factor_model.log_pi,\n",
    "            )\n",
    "            + np.multiply(\n",
    "                1 - self.lambda_matrix,\n",
    "                binary_latent_factor_model.log_one_minus_pi,\n",
    "            )\n",
    "        )\n",
    "        return expectation_log_p_x_given_s_theta + expectation_log_p_s_given_theta\n",
    "\n",
    "    def _compute_approximation_model_entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the model entropy\n",
    "\n",
    "        :return: model entropy\n",
    "        \"\"\"\n",
    "        return -np.sum(\n",
    "            np.multiply(\n",
    "                self.lambda_matrix,\n",
    "                self.log_lambda_matrix,\n",
    "            )\n",
    "            + np.multiply(\n",
    "                1 - self.lambda_matrix,\n",
    "                self.log_one_minus_lambda_matrix,\n",
    "            )\n",
    "        )\n"
   ],
   "id": "ddc0dba78a3ddf53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:10.139907Z",
     "start_time": "2024-12-25T17:02:10.134715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def m_step(X, ES, ESS):\n",
    "    \"\"\"\n",
    "    mu, sigma, pie = MStep(X,ES,ESS)\n",
    "\n",
    "    Inputs:\n",
    "    -----------------\n",
    "           X: shape (N, D) data matrix\n",
    "          ES: shape (N, K) E_q[s]\n",
    "         ESS: shape (K, K) sum over data points of E_q[ss'] (N, K, K)\n",
    "                           if E_q[ss'] is provided, the sum over N is done for you.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "          mu: shape (D, K) matrix of means in p(y|{s_i},mu,sigma)\n",
    "       sigma: shape (,)    standard deviation in same\n",
    "         pie: shape (1, K) vector of parameters specifying generative distribution for s\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    if ES.shape[0] != N:\n",
    "        raise TypeError('ES must have the same number of rows as X')\n",
    "    K = ES.shape[1]\n",
    "    if ESS.shape == (N, K, K):\n",
    "        ESS = np.sum(ESS, axis=0)\n",
    "    if ESS.shape != (K, K):\n",
    "        raise TypeError('ESS must be square and have the same number of columns as ES')\n",
    "\n",
    "    mu = np.dot(np.dot(np.linalg.inv(ESS), ES.T), X).T\n",
    "    sigma = np.sqrt((np.trace(np.dot(X.T, X)) + np.trace(np.dot(np.dot(mu.T, mu), ESS))\n",
    "                     - 2 * np.trace(np.dot(np.dot(ES.T, X), mu))) / (N * D))\n",
    "    pie = np.mean(ES, axis=0, keepdims=True)\n",
    "\n",
    "    return mu, sigma, pie\n"
   ],
   "id": "e5d5c1cecdc99240",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:10.172871Z",
     "start_time": "2024-12-25T17:02:10.160743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "class MessagePassingApproximation(AbstractBinaryLatentFactorApproximation):\n",
    "    \"\"\"\n",
    "    bernoulli_parameter_matrix (theta): matrix of parameters bernoulli_parameter_matrix[n, i, j]\n",
    "                off diagonals corresponds to \\tilda{g}_{ij, \\neg s_i}(s_j) for data point n\n",
    "                diagonals correspond to \\tilda{f}_{i}(s_i)\n",
    "                (number_of_points, number_of_latent_variables, number_of_latent_variables)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bernoulli_parameter_matrix: np.ndarray):\n",
    "        self.bernoulli_parameter_matrix = bernoulli_parameter_matrix\n",
    "\n",
    "    @property\n",
    "    def lambda_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Aggregate messages and compute parameter for Bernoulli distribution\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lambda_matrix = 1 / (1 + np.exp(-self.natural_parameter_matrix.sum(axis=1)))\n",
    "        lambda_matrix[lambda_matrix == 0] = 1e-10\n",
    "        lambda_matrix[lambda_matrix == 1] = 1 - 1e-10\n",
    "        return lambda_matrix\n",
    "\n",
    "    @property\n",
    "    def natural_parameter_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The matrix containing natural parameters (eta) of each factor\n",
    "                off diagonals corresponds to \\tilda{g}_{ij, \\neg s_i}(s_j) for data point n\n",
    "                diagonals correspond to \\tilda{f}_{i}(s_i)\n",
    "                (number_of_points, number_of_latent_variables, number_of_latent_variables)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.log(\n",
    "            np.divide(\n",
    "                self.bernoulli_parameter_matrix, 1 - self.bernoulli_parameter_matrix\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def aggregate_incoming_binary_factor_messages(\n",
    "        self, node_index: int, excluded_node_index: int\n",
    "    ) -> np.ndarray:\n",
    "        # (number_of_points, )\n",
    "        #  exclude message from excluded_node_index -> node_index\n",
    "        return (\n",
    "            np.sum(\n",
    "                self.natural_parameter_matrix[:, :excluded_node_index, node_index],\n",
    "                axis=1,\n",
    "            )\n",
    "            + np.sum(\n",
    "                self.natural_parameter_matrix[:, excluded_node_index + 1 :, node_index],\n",
    "                axis=1,\n",
    "            )\n",
    "        ).reshape(\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_bernoulli_parameter(\n",
    "        natural_parameter_matrix: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        bernoulli_parameter = 1 / (1 + np.exp(-natural_parameter_matrix))\n",
    "        bernoulli_parameter[bernoulli_parameter == 0] = 1e-10\n",
    "        bernoulli_parameter[bernoulli_parameter == 1] = 1 - 1e-10\n",
    "        return bernoulli_parameter\n",
    "\n",
    "    def variational_expectation_step(\n",
    "        self, x: np.ndarray, binary_latent_factor_model: BoltzmannMachine\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Iteratively update singleton and binary factors\n",
    "        :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "        :param binary_latent_factor_model: a binary_latent_factor_model\n",
    "        :return: free energies after each update\n",
    "        \"\"\"\n",
    "        free_energy = [self.compute_free_energy(x, binary_latent_factor_model)]\n",
    "        for i in range(self.k):\n",
    "            # singleton factor update\n",
    "            natural_parameter_ii = self.calculate_singleton_message_update(\n",
    "                boltzmann_machine=binary_latent_factor_model,\n",
    "                x=x,\n",
    "                i=i,\n",
    "            )\n",
    "            self.bernoulli_parameter_matrix[\n",
    "                :, i, i\n",
    "            ] = self.calculate_bernoulli_parameter(natural_parameter_ii)\n",
    "            free_energy.append(self.compute_free_energy(x, binary_latent_factor_model))\n",
    "\n",
    "            for j in range(i):\n",
    "                # binary factor update\n",
    "                natural_parameter_ij = self.calculate_binary_message_update(\n",
    "                    boltzmann_machine=binary_latent_factor_model,\n",
    "                    x=x,\n",
    "                    i=i,\n",
    "                    j=j,\n",
    "                )\n",
    "                self.bernoulli_parameter_matrix[\n",
    "                    :, i, j\n",
    "                ] = self.calculate_bernoulli_parameter(natural_parameter_ij)\n",
    "                natural_parameter_ji = self.calculate_binary_message_update(\n",
    "                    boltzmann_machine=binary_latent_factor_model,\n",
    "                    x=x,\n",
    "                    i=j,\n",
    "                    j=i,\n",
    "                )\n",
    "                self.bernoulli_parameter_matrix[\n",
    "                    :, j, i\n",
    "                ] = self.calculate_bernoulli_parameter(natural_parameter_ji)\n",
    "                free_energy.append(\n",
    "                    self.compute_free_energy(x, binary_latent_factor_model)\n",
    "                )\n",
    "        return free_energy\n",
    "\n",
    "    def calculate_binary_message_update(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        boltzmann_machine: BoltzmannMachine,\n",
    "        i: int,\n",
    "        j: int,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate new parameters for a binary factored message.\n",
    "\n",
    "        :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "        :param boltzmann_machine: Boltzmann machine model\n",
    "        :param i: starting node for the message\n",
    "        :param j: ending node for the message\n",
    "        :return: new parameter from aggregating incoming messages\n",
    "        \"\"\"\n",
    "        natural_parameter_i_not_j = boltzmann_machine.b_index(\n",
    "            x=x, node_index=i\n",
    "        ) + self.aggregate_incoming_binary_factor_messages(\n",
    "            node_index=i, excluded_node_index=j\n",
    "        )\n",
    "        w_i_j = boltzmann_machine.w_matrix_index(i, j)\n",
    "        return np.log(1 + np.exp(w_i_j + natural_parameter_i_not_j)) - np.log(\n",
    "            1 + np.exp(natural_parameter_i_not_j)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_singleton_message_update(\n",
    "        x: np.ndarray,\n",
    "        boltzmann_machine: BoltzmannMachine,\n",
    "        i: int,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the parameter update for the singleton message.\n",
    "        Note that this does not require any approximation.\n",
    "\n",
    "        :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "        :param boltzmann_machine: Boltzmann machine model\n",
    "        :param i: node to update\n",
    "        :return: new parameter\n",
    "        \"\"\"\n",
    "        return boltzmann_machine.b_index(x=x, node_index=i)\n",
    "\n",
    "\n",
    "def init_message_passing(k: int, n: int) -> MessagePassingApproximation:\n",
    "    \"\"\"\n",
    "    Message passing initialisation\n",
    "\n",
    "    :param k: number of latent variables\n",
    "    :param n: number of data points\n",
    "    :return: message passing\n",
    "    \"\"\"\n",
    "    bernoulli_parameter_matrix = np.random.random(size=(n, k, k))\n",
    "    return MessagePassingApproximation(bernoulli_parameter_matrix)\n"
   ],
   "id": "c255d7fcf66531db",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:33.749285Z",
     "start_time": "2024-12-25T17:02:33.740597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "class BoltzmannMachine(BinaryLatentFactorModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu: np.ndarray,\n",
    "        sigma: float,\n",
    "        pi: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Binary latent factor model with Boltzmann Machine terms\n",
    "        \"\"\"\n",
    "        super().__init__(mu, sigma, pi)\n",
    "\n",
    "    @property\n",
    "    def w_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Weight matrix of the Boltzmann machine\n",
    "\n",
    "        :return: matrix of weights (number_of_latent_variables, number_of_latent_variables)\n",
    "        \"\"\"\n",
    "        return -self.precision * (self.mu.T @ self.mu)\n",
    "\n",
    "    def w_matrix_index(self, i, j) -> float:\n",
    "        \"\"\"\n",
    "        Weight matrix at a specific index\n",
    "\n",
    "        :param i: row index\n",
    "        :param j: column index\n",
    "        :return: weight value\n",
    "        \"\"\"\n",
    "        return -self.precision * (self.mu[:, i] @ self.mu[:, j])\n",
    "\n",
    "    def b(self, x) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        b term in the Boltzmann machine for all data points\n",
    "\n",
    "        :param x: design matrix (number_of_points, number_of_dimensions)\n",
    "        :return: matrix of shape (number_of_points, number_of_latent_variables)\n",
    "        \"\"\"\n",
    "        return -(\n",
    "            self.precision * x @ self.mu\n",
    "            + self.log_pi_ratio\n",
    "            - 0.5 * self.precision * np.multiply(self.mu, self.mu).sum(axis=0)\n",
    "        )\n",
    "\n",
    "    def b_index(self, x, node_index) -> float:\n",
    "        \"\"\"\n",
    "        b term for a specific node in the Boltzmann machine for all data points\n",
    "\n",
    "        :param x: design matrix (number_of_points, number_of_dimensions)\n",
    "        :param node_index: node index\n",
    "        :return: vector of shape (number_of_points, 1)\n",
    "        \"\"\"\n",
    "        return -(\n",
    "            self.precision * x @ self.mu[:, node_index]\n",
    "            + (self.log_pi[0, node_index] - self.log_one_minus_pi[0, node_index])\n",
    "            - 0.5 * self.precision * self.mu[:, node_index] @ self.mu[:, node_index]\n",
    "        ).reshape(\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def log_pi_ratio(self) -> np.ndarray:\n",
    "        return self.log_pi - self.log_one_minus_pi\n",
    "\n",
    "\n",
    "def init_boltzmann_machine(\n",
    "    x: np.ndarray,\n",
    "    binary_latent_factor_approximation: AbstractBinaryLatentFactorApproximation,\n",
    ") -> BinaryLatentFactorModel:\n",
    "    \"\"\"\n",
    "    Initialise by running a maximisation step with the parameters of the binary latent factor approximation\n",
    "\n",
    "    :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "    :param binary_latent_factor_approximation: a binary_latent_factor_approximation\n",
    "    :return: an initialised Boltzmann machine model\n",
    "    \"\"\"\n",
    "    mu, sigma, pi = BinaryLatentFactorModel.calculate_maximisation_parameters(\n",
    "        x, binary_latent_factor_approximation\n",
    "    )\n",
    "    return BoltzmannMachine(mu=mu, sigma=sigma, pi=pi)\n"
   ],
   "id": "ae1bd2432c12f4b9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:31.391828Z",
     "start_time": "2024-12-25T17:02:31.384634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "class BinaryLatentFactorModel(AbstractBinaryLatentFactorModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu: np.ndarray,\n",
    "        sigma: float,\n",
    "        pi: np.ndarray,\n",
    "    ):\n",
    "        self._mu = mu  # (number_of_dimensions, number_of_latent_variables)\n",
    "        self._sigma = sigma\n",
    "        self._pi = pi  # (1, number_of_latent_variables)\n",
    "\n",
    "    @property\n",
    "    def mu(self):\n",
    "        return self._mu\n",
    "\n",
    "    @mu.setter\n",
    "    def mu(self, value):\n",
    "        self._mu = value\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return self._sigma\n",
    "\n",
    "    @sigma.setter\n",
    "    def sigma(self, value):\n",
    "        self._sigma = value\n",
    "\n",
    "    @property\n",
    "    def pi(self):\n",
    "        return self._pi\n",
    "\n",
    "    @pi.setter\n",
    "    def pi(self, value):\n",
    "        self._pi = value\n",
    "\n",
    "    @property\n",
    "    def variance(self) -> float:\n",
    "        return self.sigma**2\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_maximisation_parameters(\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: AbstractBinaryLatentFactorApproximation,\n",
    "    ) -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "        return m_step(\n",
    "            X=x,\n",
    "            ES=binary_latent_factor_approximation.expectation_s,\n",
    "            ESS=binary_latent_factor_approximation.expectation_ss,\n",
    "        )\n",
    "\n",
    "    def maximisation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: AbstractBinaryLatentFactorApproximation,\n",
    "    ) -> None:\n",
    "        mu, sigma, pi = self.calculate_maximisation_parameters(\n",
    "            x, binary_latent_factor_approximation\n",
    "        )\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.pi = pi\n",
    "\n",
    "\n",
    "def init_binary_latent_factor_model(\n",
    "    x: np.ndarray,\n",
    "    binary_latent_factor_approximation: AbstractBinaryLatentFactorApproximation,\n",
    ") -> BinaryLatentFactorModel:\n",
    "    \"\"\"\n",
    "    Initialise by running a maximisation step with the parameters of the binary latent factor approximation\n",
    "\n",
    "    :param x: data matrix (number_of_points, number_of_dimensions)\n",
    "    :param binary_latent_factor_approximation: a binary_latent_factor_approximation\n",
    "    :return: an initialised binary latent factor model\n",
    "    \"\"\"\n",
    "    mu, sigma, pi = BinaryLatentFactorModel.calculate_maximisation_parameters(\n",
    "        x, binary_latent_factor_approximation\n",
    "    )\n",
    "    return BinaryLatentFactorModel(mu, sigma, pi)\n"
   ],
   "id": "294ff667a2edc24",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:02:28.211202Z",
     "start_time": "2024-12-25T17:02:28.203714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from src.models.binary_latent_factor_approximations.abstract_binary_latent_factor_approximation import (\n",
    "        AbstractBinaryLatentFactorApproximation,\n",
    "    )\n",
    "\n",
    "\n",
    "class AbstractBinaryLatentFactorModel(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def mu(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        matrix of means (number_of_dimensions, number_of_latent_variables)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def variance(self) -> float:\n",
    "        \"\"\"\n",
    "        gaussian noise parameter\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def pi(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        (1, number_of_latent_variables)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def maximisation_step(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        binary_latent_factor_approximation: AbstractBinaryLatentFactorApproximation,\n",
    "    ) -> None:\n",
    "        pass\n",
    "\n",
    "    def mu_exclude(self, exclude_latent_index: int) -> np.ndarray:\n",
    "        return np.concatenate(  # (number_of_dimensions, number_of_latent_variables-1)\n",
    "            (self.mu[:, :exclude_latent_index], self.mu[:, exclude_latent_index + 1 :]),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def log_pi(self) -> np.ndarray:\n",
    "        return np.log(self.pi)\n",
    "\n",
    "    @property\n",
    "    def log_one_minus_pi(self) -> np.ndarray:\n",
    "        return np.log(1 - self.pi)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return 1 / self.variance\n",
    "\n",
    "    @property\n",
    "    def d(self) -> int:\n",
    "        return self.mu.shape[0]\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self.mu.shape[1]\n"
   ],
   "id": "2c76560008482e87",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:12:30.075719Z",
     "start_time": "2024-12-25T17:12:30.067653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def run(x: np.ndarray, k: int, em_iterations: int, save_path: str) -> None:\n",
    "    n = x.shape[0]\n",
    "    message_passing = init_message_passing(k, n)\n",
    "    boltzmann_machine = init_boltzmann_machine(x, message_passing)\n",
    "\n",
    "    # pre-training features plot\n",
    "    fig, axes = plt.subplots(1, k, figsize=(k * 2, 2))\n",
    "    for i in range(k):\n",
    "        axes[i].imshow(boltzmann_machine.mu[:, i].reshape(4, 4), cmap='gray', interpolation='none')\n",
    "        axes[i].set_title(f\"Feature {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(\"Initial Features (Loopy BP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + \"-init-latent-factors\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # EM\n",
    "    message_passing, boltzmann_machine, free_energy = learn_binary_factors(\n",
    "        x=x,\n",
    "        k=k,\n",
    "        em_iterations=em_iterations,\n",
    "        binary_latent_factor_model=boltzmann_machine,\n",
    "        binary_latent_factor_approximation=message_passing,\n",
    "    )\n",
    "\n",
    "    # post training features plot\n",
    "    fig, axes = plt.subplots(1, k, figsize=(k * 2, 2))\n",
    "    for i in range(k):\n",
    "        axes[i].imshow(boltzmann_machine.mu[:, i].reshape(4, 4), cmap='gray', interpolation='none')\n",
    "        axes[i].set_title(f\"Feature {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(\"Learned Features (Loopy BP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + \"-latent-factors\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # free energy plot\n",
    "    plt.title(\"Free Energy (Loopy BP)\")\n",
    "    plt.xlabel(\"t (EM steps)\")\n",
    "    plt.ylabel(\"Free Energy\")\n",
    "    plt.plot(free_energy)\n",
    "    plt.savefig(save_path + \"-free-energy\", bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ],
   "id": "5cd28f1b8c99c691",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:12:30.611036Z",
     "start_time": "2024-12-25T17:12:30.602752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_converge(\n",
    "    free_energies: List[float],\n",
    "    current_lambda_matrix: np.ndarray,\n",
    "    previous_lambda_matrix: np.ndarray,\n",
    "    free_energy_threshold: float = 1e-6,\n",
    "    lambda_threshold: float = 1e-6,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether the algorithm has converged based on changes in free energy\n",
    "    and the lambda matrix.\n",
    "\n",
    "    Convergence is achieved if the change in free energy between the last two iterations\n",
    "    is below a specified threshold and the change in the lambda matrix (measured by\n",
    "    the Frobenius norm) is also below a specified threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    free_energies : List[float]\n",
    "        List of free energy values recorded at each iteration.\n",
    "    current_lambda_matrix : np.ndarray\n",
    "        The current lambda matrix after the latest iteration.\n",
    "    previous_lambda_matrix : np.ndarray\n",
    "        The lambda matrix from the previous iteration.\n",
    "    free_energy_threshold : float, optional\n",
    "        Threshold for the change in free energy to determine convergence, by default 1e-6.\n",
    "    lambda_threshold : float, optional\n",
    "        Threshold for the change in the lambda matrix (Frobenius norm) to determine convergence,\n",
    "        by default 1e-6.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if both the change in free energy and the change in lambda matrix are below\n",
    "        their respective thresholds, indicating convergence. Otherwise, False.\n",
    "    \"\"\"\n",
    "    if len(free_energies) < 2:\n",
    "        # Not enough data to determine convergence\n",
    "        return False\n",
    "\n",
    "    # Calculate the absolute change in free energy\n",
    "    free_energy_change = abs(free_energies[-1] - free_energies[-2])\n",
    "\n",
    "    # Calculate the Frobenius norm of the change in lambda matrix\n",
    "    lambda_change = np.linalg.norm(current_lambda_matrix - previous_lambda_matrix)\n",
    "\n",
    "    # Check if both changes are below their respective thresholds\n",
    "    return (free_energy_change <= free_energy_threshold) and (lambda_change <= lambda_threshold)\n",
    "\n",
    "\n",
    "def learn_binary_factors(\n",
    "    x: np.ndarray,\n",
    "    k: int,\n",
    "    em_iterations: int,\n",
    "    binary_latent_factor_model: 'VariationalBayes',\n",
    "    binary_latent_factor_approximation: 'MeanFieldApproximation',\n",
    ") -> Tuple['MeanFieldApproximation', 'VariationalBayes', List[float]]:\n",
    "    \"\"\"\n",
    "    Perform the Expectation-Maximization (EM) algorithm to learn binary latent factors.\n",
    "\n",
    "    This function iteratively performs the E-step and M-step to optimize the\n",
    "    variational approximation of binary latent factors and update the\n",
    "    variational Bayes model. It records the free energy at each iteration to\n",
    "    monitor convergence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_dimensions), where n_samples is the\n",
    "        number of data points and n_dimensions is the number of observed dimensions.\n",
    "    em_iterations : int\n",
    "        Maximum number of EM iterations to perform.\n",
    "    binary_latent_factor_model : VariationalBayes\n",
    "        An instance of VariationalBayes representing the current model.\n",
    "    binary_latent_factor_approximation : MeanFieldApproximation\n",
    "        An instance of MeanFieldApproximation representing the current variational\n",
    "        approximation of the binary latent factors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[MeanFieldApproximation, VariationalBayes, List[float]]\n",
    "        A tuple containing:\n",
    "        - The updated MeanFieldApproximation instance.\n",
    "        - The updated VariationalBayes model.\n",
    "        - A list of free energy values recorded at each EM iteration.\n",
    "    \"\"\"\n",
    "    # Initialize the list of free energies with the initial free energy\n",
    "    free_energies: List[float] = [\n",
    "        binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for iteration in range(1, em_iterations + 1):\n",
    "        # Store the previous lambda matrix for convergence checking\n",
    "        previous_lambda_matrix = np.copy(binary_latent_factor_approximation.lambda_matrix)\n",
    "\n",
    "        # E-step: Update the variational approximation (lambda matrix)\n",
    "        free_energy_history = binary_latent_factor_approximation.variational_expectation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_model=binary_latent_factor_model,\n",
    "        )\n",
    "\n",
    "        # M-step: Update the variational Bayes model parameters\n",
    "        binary_latent_factor_model.maximisation_step(\n",
    "            x=x,\n",
    "            binary_latent_factor_approximation=binary_latent_factor_approximation,\n",
    "        )\n",
    "\n",
    "        # Compute and record the new free energy\n",
    "        current_free_energy = binary_latent_factor_approximation.compute_free_energy(\n",
    "            x, binary_latent_factor_model\n",
    "        )\n",
    "        free_energies.append(current_free_energy)\n",
    "\n",
    "        # Check for convergence\n",
    "        if is_converge(\n",
    "            free_energies=free_energies,\n",
    "            current_lambda_matrix=binary_latent_factor_approximation.lambda_matrix,\n",
    "            previous_lambda_matrix=previous_lambda_matrix,\n",
    "        ):\n",
    "            print(f\"current K = {k},\"\n",
    "                  f\" Convergence achieved at iteration {iteration},\"\n",
    "                  f\" Free Energy at Convergence: {current_free_energy}.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return binary_latent_factor_approximation, binary_latent_factor_model, free_energies"
   ],
   "id": "e8d4b00a4765abb2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T17:13:49.271563Z",
     "start_time": "2024-12-25T17:12:31.387074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dataclasses import asdict\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Constants for output directories and random seed\n",
    "OUTPUTS_FOLDER = \"LoopyBP\"\n",
    "DEFAULT_SEED = 43\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(DEFAULT_SEED)\n",
    "\n",
    "    if not os.path.exists(OUTPUTS_FOLDER):\n",
    "        os.makedirs(OUTPUTS_FOLDER)\n",
    "\n",
    "\n",
    "    number_of_images = 2000\n",
    "    x = data\n",
    "    k = 8\n",
    "    em_iterations = 100\n",
    "    e_maximum_steps = 50\n",
    "    e_convergence_criterion = 0\n",
    "\n",
    "\n",
    "    Q6_OUTPUT_FOLDER = os.path.join(OUTPUTS_FOLDER, \"q6\")\n",
    "    if not os.path.exists(Q6_OUTPUT_FOLDER):\n",
    "        os.makedirs(Q6_OUTPUT_FOLDER)\n",
    "    run(x, k, em_iterations, save_path=os.path.join(Q6_OUTPUT_FOLDER, \"all\"))"
   ],
   "id": "44a08f38794c2f03",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/wz5v3z6x7_gdgjx4t9zvjzxc0000gn/T/ipykernel_52814/3139600142.py:134: RuntimeWarning: overflow encountered in exp\n",
      "  return np.log(1 + np.exp(w_i_j + natural_parameter_i_not_j)) - np.log(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef9b259bcdea6328"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
